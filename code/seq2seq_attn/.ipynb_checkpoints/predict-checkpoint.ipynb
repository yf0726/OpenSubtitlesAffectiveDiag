{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from model import Options, Seq2SeqAttn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type = str, default = './pre-data/',\n",
    "                    help = 'the directory to the data')\n",
    "\n",
    "parser.add_argument('--word_embeddings_path', type = str, default = './pre-data/word_embeddings.npy',\n",
    "                    help = 'the directory to the pre-trained word embeddings')\n",
    "parser.add_argument('--num_epochs', type = int, default = 10,\n",
    "                    help = 'the number of epochs to train the data')\n",
    "parser.add_argument('--batch_size', type = int, default = 1,\n",
    "                    help = 'the batch size')\n",
    "parser.add_argument('--learning_rate', type = float, default = 0.001,\n",
    "                    help = 'the learning rate')\n",
    "parser.add_argument('--beam_width', type = int, default = 256,\n",
    "                    help = 'the beam width when decoding')\n",
    "parser.add_argument('--word_embed_size', type = int, default = 256,\n",
    "                    help = 'the size of word embeddings')\n",
    "parser.add_argument('--n_hidden_units_enc', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of encoder')\n",
    "parser.add_argument('--n_hidden_units_dec', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of decoder')\n",
    "parser.add_argument('--attn_depth', type = int, default = 128,\n",
    "                    help = 'attention depth')\n",
    "parser.add_argument('--restore_path', type = str, default = './model_dailydialog_rf',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--restore_epoch', type = int, default = 10,\n",
    "                    help = 'the epoch to restore')\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        return my_set\n",
    "    test_set = load_np_files(os.path.join(data_path, 'test'))\n",
    "    # dictionary index of words\n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "    with open(os.path.join(data_path, 'id2token.pickle'), 'rb') as file:\n",
    "        id2token = pickle.load(file)\n",
    "    return test_set, token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, uttr_len, id2token): # ?\n",
    "    tokens = []\n",
    "    if uttr_len is not None:\n",
    "        for i in range(uttr_len):\n",
    "            if id2token[ids[i]] != '<eos>' and id2token[ids[i]] != '<go>':\n",
    "                tokens.append(id2token[ids[i]])\n",
    "    else:\n",
    "        i = 0\n",
    "        while i < len(ids) and id2token[ids[i]] != '<eos>':\n",
    "            tokens.append(id2token[ids[i]])\n",
    "            i += 1\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/word_embeddings:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "Restoring a pre-trained model from ./model_dailydialog_rf/model_epoch_010.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from ./model_dailydialog_rf/model_epoch_010.ckpt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_set, token2id, id2token = read_data(args.data_path)\n",
    "    max_uttr_len_enc = test_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = test_set['dec_input'].shape[1]\n",
    "\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "\n",
    "    options = Options(mode = 'PREDICT',\n",
    "                      num_epochs = args.num_epochs,\n",
    "                      batch_size = args.batch_size,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      vocab_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    model.restore(os.path.join(args.restore_path, 'model_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    prediction = model.predict(test_set['enc_input'], test_set['enc_input_len'])\n",
    "\n",
    "    pred_S2S = []\n",
    "    f = open('./pre-data/test/pred_S2S.txt', 'w', encoding = 'utf-8')\n",
    "    N = (test_set['enc_input'].shape[0] // args.batch_size) * args.batch_size\n",
    "    for i in range(N):\n",
    "        f.write('HISTORY:\\n')\n",
    "        uttr = ids_to_sentence(test_set['enc_input'][i,:], test_set['enc_input_len'][i], id2token)\n",
    "        f.write('- {}\\n'.format(uttr))\n",
    "        f.write('LABEL:\\n')\n",
    "        label = ids_to_sentence(test_set['target'][i,:], test_set['dec_input_len'][i], id2token)\n",
    "        f.write('- {}\\n'.format(label))\n",
    "        f.write('PREDICTION:\\n')\n",
    "        pred = ids_to_sentence(prediction[i//args.batch_size][i%args.batch_size,:,0], None, id2token)\n",
    "        f.write('- {}\\n\\n'.format(pred))\n",
    "        pred_S2S.append(pred)\n",
    "    f.close()\n",
    "    with open('./pre-data/test/pred_S2S.pickle', 'wb') as f:\n",
    "        pickle.dump(pred_S2S, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just finish the job .',\n",
       " 'not exactly .',\n",
       " 'no , thanks .',\n",
       " \"i 'm dead sexy .\",\n",
       " 'watch out !',\n",
       " 'later ..',\n",
       " 'telugu medium ?',\n",
       " 'later ..',\n",
       " 'get back to work .',\n",
       " 'huh ?',\n",
       " \"bet you wo n't be laughing soon .\",\n",
       " 'sure .',\n",
       " 'again ?',\n",
       " 'answer me !',\n",
       " 'volcanic eruption !',\n",
       " 'yeah .',\n",
       " 'me .',\n",
       " 'listen .',\n",
       " 'yeah .',\n",
       " \"it 's none of your concern .\",\n",
       " 'mind if i come in ?',\n",
       " \"i 'm dead sexy .\",\n",
       " 'great .',\n",
       " \"have n't seen him since friday .\",\n",
       " 'huh ?',\n",
       " 'there will be more victims .',\n",
       " 'good to see you .',\n",
       " 'yeah .',\n",
       " 'lf you wanted',\n",
       " 'yeah .',\n",
       " 'yeah .',\n",
       " 'give me moment , hmm ?',\n",
       " 'sure .',\n",
       " 'i was in the city .',\n",
       " 'later ..',\n",
       " 'volcanic eruption !',\n",
       " 'i was just doing my job .',\n",
       " 'listen .',\n",
       " 'yeah .',\n",
       " 'huh ?',\n",
       " 'mr president , in twelve hours ...',\n",
       " 'mommy .',\n",
       " 'get back to work .',\n",
       " 'get back to work .',\n",
       " 'half-robot .',\n",
       " 'yeah .',\n",
       " 'yeah .',\n",
       " 'me .',\n",
       " 'huh ?',\n",
       " 'another bus wil come right ..',\n",
       " 'ok .',\n",
       " 'get ready !',\n",
       " 'good to see you guys .',\n",
       " 'volcanic eruption !',\n",
       " 'telugu medium ?',\n",
       " 'yeah .',\n",
       " 'great .',\n",
       " 'listen .',\n",
       " 'it thinks so .',\n",
       " 'answer me !',\n",
       " 'hold on .',\n",
       " 'hayama !',\n",
       " \"i do n't know , sir .\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
