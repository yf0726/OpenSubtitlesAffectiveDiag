{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from model import Options, Seq2SeqAttn\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.seq2seq import tile_batch\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type = str, default = '../pre-data/',\n",
    "                    help = 'the directory to the data')\n",
    "\n",
    "parser.add_argument('--word_embeddings_path', type = str, default = '../pre-data/word_embeddings.npy',\n",
    "                    help = 'the directory to the pre-trained word embeddings')\n",
    "parser.add_argument('--VAD_path', type = str, default = '../pre-data/VAD.npy',\n",
    "                    help = 'the directory to VAD')\n",
    "parser.add_argument('--tf_path', type = str, default = '../pre-data/tf.npy',\n",
    "                    help = 'the directory to term frequency')\n",
    "parser.add_argument('--VAD_loss_path', type = str, default = '../pre-data/VAD_loss.npy',\n",
    "                    help = 'the directory to VAD loss for each word')\n",
    "parser.add_argument('--ti_path', type = str, default = '../pre-data/mu_li.npy',\n",
    "                    help = 'the directory to term importance')\n",
    "\n",
    "parser.add_argument('--num_epochs', type = int, default = 1,\n",
    "                    help = 'the number of epochs to train the data')\n",
    "parser.add_argument('--batch_size', type = int, default = 64,\n",
    "                    help = 'the batch size')\n",
    "parser.add_argument('--learning_rate', type = float, default = 0.0001,\n",
    "                    help = 'the learning rate')\n",
    "parser.add_argument('--beam_width', type = int, default = 256,\n",
    "                    help = 'the beam width when decoding')\n",
    "parser.add_argument('--word_embed_size', type = int, default = 256,\n",
    "                    help = 'the size of word embeddings')\n",
    "parser.add_argument('--n_hidden_units_enc', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of encoder')\n",
    "parser.add_argument('--n_hidden_units_dec', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of decoder')\n",
    "# ? attn_depth\n",
    "parser.add_argument('--attn_depth', type = int, default = 128,\n",
    "                    help = 'attention depth')\n",
    "parser.add_argument('--restore_path', type = str, default = '../model_dailydialog_rf',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--restore_epoch', type = int, default = 5,\n",
    "                    help = 'the epoch to restore')\n",
    "\n",
    "parser.add_argument('--save_path', type = str, default = '../model_dailydialog_rf',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        return my_set\n",
    "    train_set = load_np_files(os.path.join(data_path, 'train'))\n",
    "    valid_set = load_np_files(os.path.join(data_path, 'validation'))\n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "\n",
    "    return train_set, valid_set, token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, token2id = read_data(args.data_path)\n",
    "max_uttr_len_enc = train_set['enc_input'].shape[1]\n",
    "max_uttr_len_dec = train_set['dec_input'].shape[1]\n",
    "\n",
    "word_embeddings = np.load(args.word_embeddings_path)\n",
    "VAD = np.load(args.VAD_path)\n",
    "termfreq = np.load(args.ti_path) # term importance\n",
    "termfreq = termfreq.reshape(-1,1)\n",
    "VAD_loss = np.load(args.VAD_loss_path)\n",
    "VAD_loss = VAD_loss.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = train_set['enc_input'][0:5,:]\n",
    "dec_input = train_set['dec_input'][0:5,:]\n",
    "\n",
    "target = train_set['target'][0:5,:]\n",
    "enc_input_len = tf.placeholder(tf.int32, shape = [args.batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = train_set['enc_input_len'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "options = Options(mode = 'PREDICT',\n",
    "                  num_epochs = args.num_epochs,\n",
    "                  batch_size = args.batch_size,\n",
    "                  learning_rate = args.learning_rate,\n",
    "                  beam_width = args.beam_width,\n",
    "                  vocab_size = len(token2id),\n",
    "                  max_uttr_len_enc = max_uttr_len_enc,\n",
    "                  max_uttr_len_dec = max_uttr_len_dec,\n",
    "                  go_index = token2id['<go>'],\n",
    "                  eos_index = token2id['<eos>'],\n",
    "                  word_embed_size = args.word_embed_size,\n",
    "                  n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                  n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                  attn_depth = args.attn_depth,\n",
    "                  word_embeddings = word_embeddings)\n",
    "model = Seq2SeqAttn(options)\n",
    "\n",
    "for var in model.tvars:\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "TensorFlow session is closed.\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n"
     ]
    }
   ],
   "source": [
    "options = Options(mode = 'TRAIN',\n",
    "                  num_epochs = args.num_epochs,\n",
    "                  batch_size = args.batch_size,\n",
    "                  learning_rate = args.learning_rate,\n",
    "                  beam_width = args.beam_width,\n",
    "                  vocab_size = len(token2id),\n",
    "                  max_uttr_len_enc = max_uttr_len_enc,\n",
    "                  max_uttr_len_dec = max_uttr_len_dec,\n",
    "                  go_index = token2id['<go>'],\n",
    "                  eos_index = token2id['<eos>'],\n",
    "                  word_embed_size = args.word_embed_size,\n",
    "                  n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                  n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                  attn_depth = args.attn_depth,\n",
    "                  word_embeddings = word_embeddings)\n",
    "model = Seq2SeqAttn(options)\n",
    "\n",
    "for var in model.tvars:\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_ckp = tf.train.latest_checkpoint(args.restore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding/attention_v (DT_FLOAT) [128]\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias (DT_FLOAT) [256]\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel (DT_FLOAT) [768,256]\n",
      "decoding/attention_wrapper/gru_cell/gates/bias (DT_FLOAT) [512]\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel (DT_FLOAT) [768,512]\n",
      "decoding/attention_wrapper/my_bahdanau_attention/attention_Wb/kernel (DT_FLOAT) [256,3]\n",
      "decoding/attention_wrapper/my_bahdanau_attention/query_layer/kernel (DT_FLOAT) [256,128]\n",
      "decoding/beta1_power (DT_FLOAT) []\n",
      "decoding/beta2_power (DT_FLOAT) []\n",
      "decoding/decoding/attention_wrapper/gru_cell/candidate/bias/Adam (DT_FLOAT) [256]\n",
      "decoding/decoding/attention_wrapper/gru_cell/candidate/bias/Adam_1 (DT_FLOAT) [256]\n",
      "decoding/decoding/attention_wrapper/gru_cell/candidate/kernel/Adam (DT_FLOAT) [768,256]\n",
      "decoding/decoding/attention_wrapper/gru_cell/candidate/kernel/Adam_1 (DT_FLOAT) [768,256]\n",
      "decoding/decoding/attention_wrapper/gru_cell/gates/bias/Adam (DT_FLOAT) [512]\n",
      "decoding/decoding/attention_wrapper/gru_cell/gates/bias/Adam_1 (DT_FLOAT) [512]\n",
      "decoding/decoding/attention_wrapper/gru_cell/gates/kernel/Adam (DT_FLOAT) [768,512]\n",
      "decoding/decoding/attention_wrapper/gru_cell/gates/kernel/Adam_1 (DT_FLOAT) [768,512]\n",
      "decoding/decoding/attention_wrapper/my_bahdanau_attention/attention_Wb/kernel/Adam (DT_FLOAT) [256,3]\n",
      "decoding/decoding/attention_wrapper/my_bahdanau_attention/attention_Wb/kernel/Adam_1 (DT_FLOAT) [256,3]\n",
      "decoding/decoding/dense/bias/Adam (DT_FLOAT) [3249]\n",
      "decoding/decoding/dense/bias/Adam_1 (DT_FLOAT) [3249]\n",
      "decoding/decoding/dense/kernel/Adam (DT_FLOAT) [256,3249]\n",
      "decoding/decoding/dense/kernel/Adam_1 (DT_FLOAT) [256,3249]\n",
      "decoding/dense/bias (DT_FLOAT) [3249]\n",
      "decoding/dense/kernel (DT_FLOAT) [256,3249]\n",
      "decoding/embedding/embedding/Adam (DT_FLOAT) [3249,256]\n",
      "decoding/embedding/embedding/Adam_1 (DT_FLOAT) [3249,256]\n",
      "decoding/encoding/rnn/gru_cell/candidate/bias/Adam (DT_FLOAT) [256]\n",
      "decoding/encoding/rnn/gru_cell/candidate/bias/Adam_1 (DT_FLOAT) [256]\n",
      "decoding/encoding/rnn/gru_cell/candidate/kernel/Adam (DT_FLOAT) [512,256]\n",
      "decoding/encoding/rnn/gru_cell/candidate/kernel/Adam_1 (DT_FLOAT) [512,256]\n",
      "decoding/encoding/rnn/gru_cell/gates/bias/Adam (DT_FLOAT) [512]\n",
      "decoding/encoding/rnn/gru_cell/gates/bias/Adam_1 (DT_FLOAT) [512]\n",
      "decoding/encoding/rnn/gru_cell/gates/kernel/Adam (DT_FLOAT) [512,512]\n",
      "decoding/encoding/rnn/gru_cell/gates/kernel/Adam_1 (DT_FLOAT) [512,512]\n",
      "decoding/memory_layer/kernel (DT_FLOAT) [256,128]\n",
      "embedding/embedding (DT_FLOAT) [3249,256]\n",
      "encoding/rnn/gru_cell/candidate/bias (DT_FLOAT) [256]\n",
      "encoding/rnn/gru_cell/candidate/kernel (DT_FLOAT) [512,256]\n",
      "encoding/rnn/gru_cell/gates/bias (DT_FLOAT) [512]\n",
      "encoding/rnn/gru_cell/gates/kernel (DT_FLOAT) [512,512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tensors_in_checkpoint_file(latest_ckp, all_tensors=False, tensor_name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.seq2seq import sequence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        return my_set\n",
    "    test_set = load_np_files(os.path.join(data_path, 'test'))\n",
    "    # dictionary index of words\n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "    with open(os.path.join(data_path, 'id2token.pickle'), 'rb') as file:\n",
    "        id2token = pickle.load(file)\n",
    "    return test_set, token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pre-data/test/prediction.pickle', 'rb') as file:\n",
    "    prediction = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction) # 28 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0].shape # 64: args.batch_size 20: uttr_len_dec 32: args.beam_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, token2id, id2token = read_data(args.data_path)\n",
    "max_uttr_len_enc = test_set['enc_input'].shape[1]\n",
    "max_uttr_len_dec = test_set['dec_input'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_uttr_len_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1807, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['enc_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enc_input', 'dec_input', 'target', 'enc_input_len', 'dec_input_len'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3237, 2536, 1281, 2918, 1780,   31, 2781, 2912,  272, 1741, 1975,\n",
       "       2864,   31,   37,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "test_set['target'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2437, 3219, 3219,  388,  374,  374,  374,  374,  374,  374,  374,\n",
       "         374,  374,  374,  374,  374,  374,  374,  374,  374,  374,  374,\n",
       "         374,  374,  374,  374,  374,  374,  374,  374,  374,  374],\n",
       "       [  37,   37, 2577, 2812,  326,  326,  326,  326,  326,  326,  326,\n",
       "         326,  326,  326,  326,  326,  326,  326,  326,  326,  326,  326,\n",
       "         326,  326,  326,  326,  326,  326,  326,  326,  326,  326],\n",
       "       [  37,   37,   37,   37, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502],\n",
       "       [  37,   37,   37,   37, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502],\n",
       "       [  37,   37,   37,   37,   37, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502],\n",
       "       [  37,   37,   37,   37,   37,   37, 2299, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37, 2299, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502, 2502],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37, 2299, 2615,\n",
       "        2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615,\n",
       "        2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615, 2615],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2502,\n",
       "        2502, 2502, 2502, 2615, 2502, 2502, 2502, 2502, 2502, 2615, 2615,\n",
       "        2502, 2502, 2502, 2502, 2502, 2502, 2502, 2615, 2502, 2502],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2615,\n",
       "        2615, 2615, 2615,  363, 2615, 2615, 2615, 2615, 2615,  363,  363,\n",
       "        2615, 2615, 2615, 2615, 2615, 2615, 2615,  363, 2615, 2615],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37,  363,\n",
       "         363,  363,  363,  363,  363,  363,  363,  363,  363,  363,  363,\n",
       "         363,  363,  363,  363,  363,  363,  363,  363,  363,  363],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37,  363,\n",
       "         363,  363,  363,  363,  363,  363,  363,  363,  363,  363,  363,\n",
       "         363,  363,  363,  363,  363,  363,  363,  363,  363,  363],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37,  363,\n",
       "         363,  363,  363, 2776,  363,  363,  363,  363,  363, 2776, 2776,\n",
       "         363,  363,  363,  363,  363,  363,  363, 2776,  363,  363],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2776,\n",
       "        2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776,\n",
       "        2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2776,\n",
       "        2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776,\n",
       "        2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2776,\n",
       "        2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776,  320,\n",
       "         320, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776, 2776],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37,  320,\n",
       "        2776, 2776,  320,  320,  320, 2776, 2776,  320, 2776,  320, 2776,\n",
       "        2776, 2776, 2776,  320,  320,  320,  320,  320, 2776,  320],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2776,\n",
       "         320,  320, 2776, 2776, 2776,  320,  320, 2776,  320, 2776, 2776,\n",
       "        2776,  320,  320, 2776, 2776, 2776, 2776, 2776,  320, 2776],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37, 2776,\n",
       "        2776, 2776,  320, 2776, 2776, 2776, 2776, 2776,  320,  320,  320,\n",
       "         320, 2776, 2776, 2776, 1402, 2776, 2776, 2776, 2776, 2776],\n",
       "       [  37,   37,   37,   37,   37,   37,   37,   37,   37,   37,  320,\n",
       "         320, 1402, 2776,  320, 1402, 1081, 2776, 1081, 2776, 2776, 2776,\n",
       "        2776, 2335, 1919, 2776, 2335, 1919, 1730, 1402, 1730, 1951]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[idx//args.batch_size][idx%args.batch_size,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-77be891e31f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/semester/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\u001b[0m in \u001b[0;36msequence_loss\u001b[0;34m(logits, targets, weights, average_across_timesteps, average_across_batch, softmax_loss_function, name)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mdimensions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mweights\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     raise ValueError(\"Logits must be a \"\n\u001b[1;32m     79\u001b[0m                      \"[batch_size x sequence_length x logits] tensor\")\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sequence_mask = tf.sequence_mask(test_set['enc_input_len'], maxlen = max_uttr_len_dec, dtype = tf.float32)\n",
    "weights = sequence_mask# * target_VAD_loss # affective objective function\n",
    "# sequence_mask: [batch_size, max_len]\n",
    "# target: [batch_size, max_len] VAD_loss: [batch_size,max_len]\n",
    "logits = prediction[idx//args.batch_size][idx%args.batch_size:idx%args.batch_size+args.batch_size,:,0]\n",
    "target = test_set['target'][idx:idx+args.batch_size]\n",
    "target = tf.constant(target)\n",
    "loss = sequence_loss(logits, target, weights)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.arange(test_set['dec_input'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_samples = idx[-1]%args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_batch_idx = np.random.randint(0,idx[-1]-left_samples,size = args.batch_size - left_samples - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1807,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([idx,last_batch_idx]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1807//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1856//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38, 3237, 2536, 1281, 2918, 1780,   31, 2781, 2912,  272, 1741,\n",
       "       1975, 2864,   31,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['dec_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2437,   37,   37, ...,   37,   37,   37],\n",
       "       [2437, 2437,   37, ...,   37,   37,   37],\n",
       "       [2776, 2776,  320, ..., 2335, 2335,  592],\n",
       "       ...,\n",
       "       [2437,   37,   37, ...,   37,   37,   37],\n",
       "       [1309, 1043, 1043, ..., 2335, 2335, 2335],\n",
       "       [ 374,  326, 2502, ...,  508,  508, 2934]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'Const_5:0' shape=(64, 20) dtype=int32>>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(logits).get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 5, 5, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1904-6)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
