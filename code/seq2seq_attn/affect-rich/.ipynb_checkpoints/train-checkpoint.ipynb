{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from model import Options, Seq2SeqAttn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type = str, default = '../pre-data/',\n",
    "                    help = 'the directory to the data')\n",
    "\n",
    "parser.add_argument('--word_embeddings_path', type = str, default = '../pre-data/word_embeddings.npy',\n",
    "                    help = 'the directory to the pre-trained word embeddings')\n",
    "parser.add_argument('--VAD_path', type = str, default = '../pre-data/VAD.npy',\n",
    "                    help = 'the directory to VAD')\n",
    "parser.add_argument('--tf_path', type = str, default = '../pre-data/tf.npy',\n",
    "                    help = 'the directory to term frequency')\n",
    "parser.add_argument('--VAD_loss_path', type = str, default = '../pre-data/VAD_loss.npy',\n",
    "                    help = 'the directory to VAD loss for each word')\n",
    "parser.add_argument('--ti_path', type = str, default = '../pre-data/mu_li.npy',\n",
    "                    help = 'the directory to term importance')\n",
    "\n",
    "parser.add_argument('--num_epochs', type = int, default = 5,\n",
    "                    help = 'the number of epochs to train the data')\n",
    "parser.add_argument('--batch_size', type = int, default = 64,\n",
    "                    help = 'the batch size')\n",
    "parser.add_argument('--learning_rate', type = float, default = 0.0001,\n",
    "                    help = 'the learning rate')\n",
    "parser.add_argument('--beam_width', type = int, default = 32,\n",
    "                    help = 'the beam width when decoding')\n",
    "parser.add_argument('--word_embed_size', type = int, default = 256,\n",
    "                    help = 'the size of word embeddings')\n",
    "parser.add_argument('--n_hidden_units_enc', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of encoder')\n",
    "parser.add_argument('--n_hidden_units_dec', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of decoder')\n",
    "# ? attn_depth\n",
    "parser.add_argument('--attn_depth', type = int, default = 128,\n",
    "                    help = 'attention depth')\n",
    "\n",
    "parser.add_argument('--restore_path_TS', type = str, default = '../model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_TS', type = str, default = '../model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_path_ST', type = str, default = '../model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_ST', type = str, default = '../model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_epoch', type = int, default = 0,\n",
    "                    help = 'the epoch to restore')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        # to check if or not to complete the last batch\n",
    "        idx = np.arange(my_set['dec_input'].shape[0])\n",
    "        left_samples = idx[-1]%args.batch_size\n",
    "        if left_samples:\n",
    "            last_batch_idx = np.random.randint(0,idx[-1]-left_samples,size = args.batch_size - left_samples - 1)\n",
    "            idx = np.concatenate([idx,last_batch_idx])\n",
    "            \n",
    "            my_set['enc_input'] = my_set['enc_input'][idx]\n",
    "            my_set['dec_input'] = my_set['dec_input'][idx]\n",
    "            my_set['target'] = my_set['target'][idx]\n",
    "            my_set['enc_input_len'] = my_set['enc_input_len'][idx]\n",
    "            my_set['dec_input_len'] = my_set['dec_input_len'][idx]\n",
    "        return my_set\n",
    "    train_set = load_np_files(os.path.join(data_path, 'train'))\n",
    "    valid_set = load_np_files(os.path.join(data_path, 'validation'))\n",
    "    \n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "    with open(os.path.join(data_path, 'id2token.pickle'), 'rb') as file:\n",
    "        id2token = pickle.load(file)\n",
    "\n",
    "    return train_set, valid_set, token2id,id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Train model maximizing P(T|S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "TensorFlow variables initialized.\n",
      "Start to train the model...\n",
      "Epoch 001/005, valid ppl = None, batch 0001/0226, train loss = 8.088951110839844\n",
      "Epoch 001/005, valid ppl = None, batch 0002/0226, train loss = 8.083484649658203\n",
      "Epoch 001/005, valid ppl = None, batch 0003/0226, train loss = 8.062005043029785\n",
      "Epoch 001/005, valid ppl = None, batch 0004/0226, train loss = 8.053374290466309\n",
      "Epoch 001/005, valid ppl = None, batch 0005/0226, train loss = 8.039032936096191\n",
      "Epoch 001/005, valid ppl = None, batch 0006/0226, train loss = 8.034972190856934\n",
      "Epoch 001/005, valid ppl = None, batch 0007/0226, train loss = 7.991415500640869\n",
      "Epoch 001/005, valid ppl = None, batch 0008/0226, train loss = 7.988171577453613\n",
      "Epoch 001/005, valid ppl = None, batch 0009/0226, train loss = 7.982520580291748\n",
      "Epoch 001/005, valid ppl = None, batch 0010/0226, train loss = 7.967978000640869\n",
      "Epoch 001/005, valid ppl = None, batch 0011/0226, train loss = 7.948551177978516\n",
      "Epoch 001/005, valid ppl = None, batch 0012/0226, train loss = 7.9295125007629395\n",
      "Epoch 001/005, valid ppl = None, batch 0013/0226, train loss = 7.916442394256592\n",
      "Epoch 001/005, valid ppl = None, batch 0014/0226, train loss = 7.911196231842041\n",
      "Epoch 001/005, valid ppl = None, batch 0015/0226, train loss = 7.898261070251465\n",
      "Epoch 001/005, valid ppl = None, batch 0016/0226, train loss = 7.8580708503723145\n",
      "Epoch 001/005, valid ppl = None, batch 0017/0226, train loss = 7.852222442626953\n",
      "Epoch 001/005, valid ppl = None, batch 0018/0226, train loss = 7.825345993041992\n",
      "Epoch 001/005, valid ppl = None, batch 0019/0226, train loss = 7.803951740264893\n",
      "Epoch 001/005, valid ppl = None, batch 0020/0226, train loss = 7.803760051727295\n",
      "Epoch 001/005, valid ppl = None, batch 0021/0226, train loss = 7.7852349281311035\n",
      "Epoch 001/005, valid ppl = None, batch 0022/0226, train loss = 7.721868515014648\n",
      "Epoch 001/005, valid ppl = None, batch 0023/0226, train loss = 7.680058002471924\n",
      "Epoch 001/005, valid ppl = None, batch 0024/0226, train loss = 7.701517105102539\n",
      "Epoch 001/005, valid ppl = None, batch 0025/0226, train loss = 7.6923723220825195\n",
      "Epoch 001/005, valid ppl = None, batch 0026/0226, train loss = 7.672466278076172\n",
      "Epoch 001/005, valid ppl = None, batch 0027/0226, train loss = 7.618685245513916\n",
      "Epoch 001/005, valid ppl = None, batch 0028/0226, train loss = 7.600850582122803\n",
      "Epoch 001/005, valid ppl = None, batch 0029/0226, train loss = 7.579299449920654\n",
      "Epoch 001/005, valid ppl = None, batch 0030/0226, train loss = 7.561405181884766\n",
      "Epoch 001/005, valid ppl = None, batch 0031/0226, train loss = 7.464028358459473\n",
      "Epoch 001/005, valid ppl = None, batch 0032/0226, train loss = 7.472137928009033\n",
      "Epoch 001/005, valid ppl = None, batch 0033/0226, train loss = 7.42427396774292\n",
      "Epoch 001/005, valid ppl = None, batch 0034/0226, train loss = 7.382117748260498\n",
      "Epoch 001/005, valid ppl = None, batch 0035/0226, train loss = 7.350508213043213\n",
      "Epoch 001/005, valid ppl = None, batch 0036/0226, train loss = 7.269228935241699\n",
      "Epoch 001/005, valid ppl = None, batch 0037/0226, train loss = 7.2506842613220215\n",
      "Epoch 001/005, valid ppl = None, batch 0038/0226, train loss = 7.188124656677246\n",
      "Epoch 001/005, valid ppl = None, batch 0039/0226, train loss = 7.0911455154418945\n",
      "Epoch 001/005, valid ppl = None, batch 0040/0226, train loss = 7.093680381774902\n",
      "Epoch 001/005, valid ppl = None, batch 0041/0226, train loss = 6.978226661682129\n",
      "Epoch 001/005, valid ppl = None, batch 0042/0226, train loss = 6.947405815124512\n",
      "Epoch 001/005, valid ppl = None, batch 0043/0226, train loss = 6.87346076965332\n",
      "Epoch 001/005, valid ppl = None, batch 0044/0226, train loss = 6.785275459289551\n",
      "Epoch 001/005, valid ppl = None, batch 0045/0226, train loss = 6.7861409187316895\n",
      "Epoch 001/005, valid ppl = None, batch 0046/0226, train loss = 6.662667274475098\n",
      "Epoch 001/005, valid ppl = None, batch 0047/0226, train loss = 6.608825206756592\n",
      "Epoch 001/005, valid ppl = None, batch 0048/0226, train loss = 6.6589250564575195\n",
      "Epoch 001/005, valid ppl = None, batch 0049/0226, train loss = 6.532827377319336\n",
      "Epoch 001/005, valid ppl = None, batch 0050/0226, train loss = 6.490325450897217\n",
      "Epoch 001/005, valid ppl = None, batch 0051/0226, train loss = 6.352170944213867\n",
      "Epoch 001/005, valid ppl = None, batch 0052/0226, train loss = 6.395638942718506\n",
      "Epoch 001/005, valid ppl = None, batch 0053/0226, train loss = 6.285877227783203\n",
      "Epoch 001/005, valid ppl = None, batch 0054/0226, train loss = 6.402032852172852\n",
      "Epoch 001/005, valid ppl = None, batch 0055/0226, train loss = 6.305295467376709\n",
      "Epoch 001/005, valid ppl = None, batch 0056/0226, train loss = 6.215288162231445\n",
      "Epoch 001/005, valid ppl = None, batch 0057/0226, train loss = 6.216071605682373\n",
      "Epoch 001/005, valid ppl = None, batch 0058/0226, train loss = 6.252995491027832\n",
      "Epoch 001/005, valid ppl = None, batch 0059/0226, train loss = 6.252245903015137\n",
      "Epoch 001/005, valid ppl = None, batch 0060/0226, train loss = 6.161623954772949\n",
      "Epoch 001/005, valid ppl = None, batch 0061/0226, train loss = 6.049840450286865\n",
      "Epoch 001/005, valid ppl = None, batch 0062/0226, train loss = 6.046827793121338\n",
      "Epoch 001/005, valid ppl = None, batch 0063/0226, train loss = 6.233355522155762\n",
      "Epoch 001/005, valid ppl = None, batch 0064/0226, train loss = 5.92840576171875\n",
      "Epoch 001/005, valid ppl = None, batch 0065/0226, train loss = 5.9939141273498535\n",
      "Epoch 001/005, valid ppl = None, batch 0066/0226, train loss = 5.96492338180542\n",
      "Epoch 001/005, valid ppl = None, batch 0067/0226, train loss = 5.900493144989014\n",
      "Epoch 001/005, valid ppl = None, batch 0068/0226, train loss = 5.8540520668029785\n",
      "Epoch 001/005, valid ppl = None, batch 0069/0226, train loss = 5.8964056968688965\n",
      "Epoch 001/005, valid ppl = None, batch 0070/0226, train loss = 6.034811973571777\n",
      "Epoch 001/005, valid ppl = None, batch 0071/0226, train loss = 5.95571756362915\n",
      "Epoch 001/005, valid ppl = None, batch 0072/0226, train loss = 5.791400909423828\n",
      "Epoch 001/005, valid ppl = None, batch 0073/0226, train loss = 5.733334064483643\n",
      "Epoch 001/005, valid ppl = None, batch 0074/0226, train loss = 5.827822208404541\n",
      "Epoch 001/005, valid ppl = None, batch 0075/0226, train loss = 5.762309551239014\n",
      "Epoch 001/005, valid ppl = None, batch 0076/0226, train loss = 5.72667121887207\n",
      "Epoch 001/005, valid ppl = None, batch 0077/0226, train loss = 5.699257850646973\n",
      "Epoch 001/005, valid ppl = None, batch 0078/0226, train loss = 5.583017826080322\n",
      "Epoch 001/005, valid ppl = None, batch 0079/0226, train loss = 5.732665061950684\n",
      "Epoch 001/005, valid ppl = None, batch 0080/0226, train loss = 5.551589488983154\n",
      "Epoch 001/005, valid ppl = None, batch 0081/0226, train loss = 5.67482328414917\n",
      "Epoch 001/005, valid ppl = None, batch 0082/0226, train loss = 5.567708492279053\n",
      "Epoch 001/005, valid ppl = None, batch 0083/0226, train loss = 5.606467247009277\n",
      "Epoch 001/005, valid ppl = None, batch 0084/0226, train loss = 5.57050085067749\n",
      "Epoch 001/005, valid ppl = None, batch 0085/0226, train loss = 5.59456205368042\n",
      "Epoch 001/005, valid ppl = None, batch 0086/0226, train loss = 5.509718894958496\n",
      "Epoch 001/005, valid ppl = None, batch 0087/0226, train loss = 5.539024353027344\n",
      "Epoch 001/005, valid ppl = None, batch 0088/0226, train loss = 5.512472629547119\n",
      "Epoch 001/005, valid ppl = None, batch 0089/0226, train loss = 5.487603187561035\n",
      "Epoch 001/005, valid ppl = None, batch 0090/0226, train loss = 5.567535400390625\n",
      "Epoch 001/005, valid ppl = None, batch 0091/0226, train loss = 5.488803386688232\n",
      "Epoch 001/005, valid ppl = None, batch 0092/0226, train loss = 5.299191474914551\n",
      "Epoch 001/005, valid ppl = None, batch 0093/0226, train loss = 5.460514545440674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/005, valid ppl = None, batch 0094/0226, train loss = 5.35126256942749\n",
      "Epoch 001/005, valid ppl = None, batch 0095/0226, train loss = 5.356368541717529\n",
      "Epoch 001/005, valid ppl = None, batch 0096/0226, train loss = 5.20646858215332\n",
      "Epoch 001/005, valid ppl = None, batch 0097/0226, train loss = 5.4400954246521\n",
      "Epoch 001/005, valid ppl = None, batch 0098/0226, train loss = 5.313283920288086\n",
      "Epoch 001/005, valid ppl = None, batch 0099/0226, train loss = 5.256808757781982\n",
      "Epoch 001/005, valid ppl = None, batch 0100/0226, train loss = 5.280642032623291\n",
      "Epoch 001/005, valid ppl = None, batch 0101/0226, train loss = 5.141192436218262\n",
      "Epoch 001/005, valid ppl = None, batch 0102/0226, train loss = 5.347930431365967\n",
      "Epoch 001/005, valid ppl = None, batch 0103/0226, train loss = 5.280808448791504\n",
      "Epoch 001/005, valid ppl = None, batch 0104/0226, train loss = 5.230621337890625\n",
      "Epoch 001/005, valid ppl = None, batch 0105/0226, train loss = 5.293386459350586\n",
      "Epoch 001/005, valid ppl = None, batch 0106/0226, train loss = 5.218543529510498\n",
      "Epoch 001/005, valid ppl = None, batch 0107/0226, train loss = 5.168047904968262\n",
      "Epoch 001/005, valid ppl = None, batch 0108/0226, train loss = 5.234711647033691\n",
      "Epoch 001/005, valid ppl = None, batch 0109/0226, train loss = 5.270486831665039\n",
      "Epoch 001/005, valid ppl = None, batch 0110/0226, train loss = 5.270707607269287\n",
      "Epoch 001/005, valid ppl = None, batch 0111/0226, train loss = 5.221654415130615\n",
      "Epoch 001/005, valid ppl = None, batch 0112/0226, train loss = 5.148416519165039\n",
      "Epoch 001/005, valid ppl = None, batch 0113/0226, train loss = 5.1282782554626465\n",
      "Epoch 001/005, valid ppl = None, batch 0114/0226, train loss = 5.279016971588135\n",
      "Epoch 001/005, valid ppl = None, batch 0115/0226, train loss = 5.322990417480469\n",
      "Epoch 001/005, valid ppl = None, batch 0116/0226, train loss = 4.958512306213379\n",
      "Epoch 001/005, valid ppl = None, batch 0117/0226, train loss = 5.085994720458984\n",
      "Epoch 001/005, valid ppl = None, batch 0118/0226, train loss = 5.066292762756348\n",
      "Epoch 001/005, valid ppl = None, batch 0119/0226, train loss = 5.209852695465088\n",
      "Epoch 001/005, valid ppl = None, batch 0120/0226, train loss = 5.082665920257568\n",
      "Epoch 001/005, valid ppl = None, batch 0121/0226, train loss = 5.121802806854248\n",
      "Epoch 001/005, valid ppl = None, batch 0122/0226, train loss = 5.159941673278809\n",
      "Epoch 001/005, valid ppl = None, batch 0123/0226, train loss = 5.236507892608643\n",
      "Epoch 001/005, valid ppl = None, batch 0124/0226, train loss = 5.240038871765137\n",
      "Epoch 001/005, valid ppl = None, batch 0125/0226, train loss = 5.261706829071045\n",
      "Epoch 001/005, valid ppl = None, batch 0126/0226, train loss = 5.065467357635498\n",
      "Epoch 001/005, valid ppl = None, batch 0127/0226, train loss = 5.239991664886475\n",
      "Epoch 001/005, valid ppl = None, batch 0128/0226, train loss = 5.146733283996582\n",
      "Epoch 001/005, valid ppl = None, batch 0129/0226, train loss = 5.141230583190918\n",
      "Epoch 001/005, valid ppl = None, batch 0130/0226, train loss = 5.178504943847656\n",
      "Epoch 001/005, valid ppl = None, batch 0131/0226, train loss = 5.107291221618652\n",
      "Epoch 001/005, valid ppl = None, batch 0132/0226, train loss = 5.145227432250977\n",
      "Epoch 001/005, valid ppl = None, batch 0133/0226, train loss = 5.117389678955078\n",
      "Epoch 001/005, valid ppl = None, batch 0134/0226, train loss = 4.96990442276001\n",
      "Epoch 001/005, valid ppl = None, batch 0135/0226, train loss = 4.899210453033447\n",
      "Epoch 001/005, valid ppl = None, batch 0136/0226, train loss = 4.996269226074219\n",
      "Epoch 001/005, valid ppl = None, batch 0137/0226, train loss = 5.010010242462158\n",
      "Epoch 001/005, valid ppl = None, batch 0138/0226, train loss = 5.144144535064697\n",
      "Epoch 001/005, valid ppl = None, batch 0139/0226, train loss = 5.104290008544922\n",
      "Epoch 001/005, valid ppl = None, batch 0140/0226, train loss = 5.005722999572754\n",
      "Epoch 001/005, valid ppl = None, batch 0141/0226, train loss = 4.7424635887146\n",
      "Epoch 001/005, valid ppl = None, batch 0142/0226, train loss = 4.784769058227539\n",
      "Epoch 001/005, valid ppl = None, batch 0143/0226, train loss = 4.825474739074707\n",
      "Epoch 001/005, valid ppl = None, batch 0144/0226, train loss = 4.853594779968262\n",
      "Epoch 001/005, valid ppl = None, batch 0145/0226, train loss = 5.006073474884033\n",
      "Epoch 001/005, valid ppl = None, batch 0146/0226, train loss = 4.915843486785889\n",
      "Epoch 001/005, valid ppl = None, batch 0147/0226, train loss = 4.868185520172119\n",
      "Epoch 001/005, valid ppl = None, batch 0148/0226, train loss = 5.125204086303711\n",
      "Epoch 001/005, valid ppl = None, batch 0149/0226, train loss = 4.9548258781433105\n",
      "Epoch 001/005, valid ppl = None, batch 0150/0226, train loss = 4.960023403167725\n",
      "Epoch 001/005, valid ppl = None, batch 0151/0226, train loss = 4.990912914276123\n",
      "Epoch 001/005, valid ppl = None, batch 0152/0226, train loss = 4.874505043029785\n",
      "Epoch 001/005, valid ppl = None, batch 0153/0226, train loss = 4.88532829284668\n",
      "Epoch 001/005, valid ppl = None, batch 0154/0226, train loss = 5.012054920196533\n",
      "Epoch 001/005, valid ppl = None, batch 0155/0226, train loss = 5.056577205657959\n",
      "Epoch 001/005, valid ppl = None, batch 0156/0226, train loss = 4.948997497558594\n",
      "Epoch 001/005, valid ppl = None, batch 0157/0226, train loss = 4.820206165313721\n",
      "Epoch 001/005, valid ppl = None, batch 0158/0226, train loss = 4.944264888763428\n",
      "Epoch 001/005, valid ppl = None, batch 0159/0226, train loss = 4.965707302093506\n",
      "Epoch 001/005, valid ppl = None, batch 0160/0226, train loss = 4.852573394775391\n",
      "Epoch 001/005, valid ppl = None, batch 0161/0226, train loss = 5.101897716522217\n",
      "Epoch 001/005, valid ppl = None, batch 0162/0226, train loss = 4.883593559265137\n",
      "Epoch 001/005, valid ppl = None, batch 0163/0226, train loss = 5.0121002197265625\n",
      "Epoch 001/005, valid ppl = None, batch 0164/0226, train loss = 4.768468856811523\n",
      "Epoch 001/005, valid ppl = None, batch 0165/0226, train loss = 4.903990745544434\n",
      "Epoch 001/005, valid ppl = None, batch 0166/0226, train loss = 4.9268412590026855\n",
      "Epoch 001/005, valid ppl = None, batch 0167/0226, train loss = 5.037749767303467\n",
      "Epoch 001/005, valid ppl = None, batch 0168/0226, train loss = 4.938910007476807\n",
      "Epoch 001/005, valid ppl = None, batch 0169/0226, train loss = 5.060636520385742\n",
      "Epoch 001/005, valid ppl = None, batch 0170/0226, train loss = 4.730030536651611\n",
      "Epoch 001/005, valid ppl = None, batch 0171/0226, train loss = 4.824508190155029\n",
      "Epoch 001/005, valid ppl = None, batch 0172/0226, train loss = 4.946712970733643\n",
      "Epoch 001/005, valid ppl = None, batch 0173/0226, train loss = 5.076220989227295\n",
      "Epoch 001/005, valid ppl = None, batch 0174/0226, train loss = 4.7740912437438965\n",
      "Epoch 001/005, valid ppl = None, batch 0175/0226, train loss = 4.862551212310791\n",
      "Epoch 001/005, valid ppl = None, batch 0176/0226, train loss = 5.037972927093506\n",
      "Epoch 001/005, valid ppl = None, batch 0177/0226, train loss = 5.082300186157227\n",
      "Epoch 001/005, valid ppl = None, batch 0178/0226, train loss = 5.048452854156494\n",
      "Epoch 001/005, valid ppl = None, batch 0179/0226, train loss = 4.921050548553467\n",
      "Epoch 001/005, valid ppl = None, batch 0180/0226, train loss = 4.9205827713012695\n",
      "Epoch 001/005, valid ppl = None, batch 0181/0226, train loss = 4.861874103546143\n",
      "Epoch 001/005, valid ppl = None, batch 0182/0226, train loss = 4.855464935302734\n",
      "Epoch 001/005, valid ppl = None, batch 0183/0226, train loss = 4.948533535003662\n",
      "Epoch 001/005, valid ppl = None, batch 0184/0226, train loss = 4.833831787109375\n",
      "Epoch 001/005, valid ppl = None, batch 0185/0226, train loss = 4.787633895874023\n",
      "Epoch 001/005, valid ppl = None, batch 0186/0226, train loss = 4.904181003570557\n",
      "Epoch 001/005, valid ppl = None, batch 0187/0226, train loss = 4.866692543029785\n",
      "Epoch 001/005, valid ppl = None, batch 0188/0226, train loss = 4.868508815765381\n",
      "Epoch 001/005, valid ppl = None, batch 0189/0226, train loss = 4.91264533996582\n",
      "Epoch 001/005, valid ppl = None, batch 0190/0226, train loss = 4.8524956703186035\n",
      "Epoch 001/005, valid ppl = None, batch 0191/0226, train loss = 4.972641944885254\n",
      "Epoch 001/005, valid ppl = None, batch 0192/0226, train loss = 4.85294246673584\n",
      "Epoch 001/005, valid ppl = None, batch 0193/0226, train loss = 4.794430255889893\n",
      "Epoch 001/005, valid ppl = None, batch 0194/0226, train loss = 4.901473045349121\n",
      "Epoch 001/005, valid ppl = None, batch 0195/0226, train loss = 4.813727378845215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/005, valid ppl = None, batch 0196/0226, train loss = 4.8175225257873535\n",
      "Epoch 001/005, valid ppl = None, batch 0197/0226, train loss = 4.835391044616699\n",
      "Epoch 001/005, valid ppl = None, batch 0198/0226, train loss = 4.809834003448486\n",
      "Epoch 001/005, valid ppl = None, batch 0199/0226, train loss = 4.967055797576904\n",
      "Epoch 001/005, valid ppl = None, batch 0200/0226, train loss = 4.860891819000244\n",
      "Epoch 001/005, valid ppl = None, batch 0201/0226, train loss = 5.029396057128906\n",
      "Epoch 001/005, valid ppl = None, batch 0202/0226, train loss = 4.871562480926514\n",
      "Epoch 001/005, valid ppl = None, batch 0203/0226, train loss = 4.971390247344971\n",
      "Epoch 001/005, valid ppl = None, batch 0204/0226, train loss = 4.822817325592041\n",
      "Epoch 001/005, valid ppl = None, batch 0205/0226, train loss = 4.6669087409973145\n",
      "Epoch 001/005, valid ppl = None, batch 0206/0226, train loss = 4.762619495391846\n",
      "Epoch 001/005, valid ppl = None, batch 0207/0226, train loss = 4.831693649291992\n",
      "Epoch 001/005, valid ppl = None, batch 0208/0226, train loss = 4.737199783325195\n",
      "Epoch 001/005, valid ppl = None, batch 0209/0226, train loss = 4.8192901611328125\n",
      "Epoch 001/005, valid ppl = None, batch 0210/0226, train loss = 4.654087066650391\n",
      "Epoch 001/005, valid ppl = None, batch 0211/0226, train loss = 4.813840389251709\n",
      "Epoch 001/005, valid ppl = None, batch 0212/0226, train loss = 4.8949360847473145\n",
      "Epoch 001/005, valid ppl = None, batch 0213/0226, train loss = 4.693599224090576\n",
      "Epoch 001/005, valid ppl = None, batch 0214/0226, train loss = 4.9212541580200195\n",
      "Epoch 001/005, valid ppl = None, batch 0215/0226, train loss = 4.769876480102539\n",
      "Epoch 001/005, valid ppl = None, batch 0216/0226, train loss = 4.813720226287842\n",
      "Epoch 001/005, valid ppl = None, batch 0217/0226, train loss = 4.708035945892334\n",
      "Epoch 001/005, valid ppl = None, batch 0218/0226, train loss = 4.699490070343018\n",
      "Epoch 001/005, valid ppl = None, batch 0219/0226, train loss = 4.661935329437256\n",
      "Epoch 001/005, valid ppl = None, batch 0220/0226, train loss = 4.853722095489502\n",
      "Epoch 001/005, valid ppl = None, batch 0221/0226, train loss = 4.807616710662842\n",
      "Epoch 001/005, valid ppl = None, batch 0222/0226, train loss = 4.9740891456604\n",
      "Epoch 001/005, valid ppl = None, batch 0223/0226, train loss = 4.74088716506958\n",
      "Epoch 001/005, valid ppl = None, batch 0224/0226, train loss = 4.671753406524658\n",
      "Epoch 001/005, valid ppl = None, batch 0225/0226, train loss = 4.841009616851807\n",
      "Epoch 001/005, valid ppl = None, batch 0226/0226, train loss = 4.762515544891357\n",
      "Saving the trained model to ../model_dailydialog_rf/model_TS/model_epoch_001.ckpt...\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0001/0226, train loss = 4.717263221740723\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0002/0226, train loss = 4.591179847717285\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0003/0226, train loss = 4.750877380371094\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0004/0226, train loss = 4.568056106567383\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0005/0226, train loss = 4.744668006896973\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0006/0226, train loss = 4.560530662536621\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0007/0226, train loss = 4.6504130363464355\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0008/0226, train loss = 4.8517746925354\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0009/0226, train loss = 4.55040979385376\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0010/0226, train loss = 4.621578693389893\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0011/0226, train loss = 4.780148983001709\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0012/0226, train loss = 4.518606185913086\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0013/0226, train loss = 4.702282905578613\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0014/0226, train loss = 4.839680194854736\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0015/0226, train loss = 4.42951774597168\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0016/0226, train loss = 4.827799320220947\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0017/0226, train loss = 4.677141189575195\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0018/0226, train loss = 4.869811534881592\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0019/0226, train loss = 4.605213642120361\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0020/0226, train loss = 4.74305534362793\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0021/0226, train loss = 4.631875991821289\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0022/0226, train loss = 4.720232009887695\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0023/0226, train loss = 4.7829718589782715\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0024/0226, train loss = 4.7497239112854\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0025/0226, train loss = 4.597597122192383\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0026/0226, train loss = 4.609029293060303\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0027/0226, train loss = 4.777203559875488\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0028/0226, train loss = 4.83831262588501\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0029/0226, train loss = 4.63091516494751\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0030/0226, train loss = 4.641523838043213\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0031/0226, train loss = 4.650959491729736\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0032/0226, train loss = 4.747866630554199\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0033/0226, train loss = 4.654918193817139\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0034/0226, train loss = 4.737489223480225\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0035/0226, train loss = 4.589657783508301\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0036/0226, train loss = 4.7832818031311035\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0037/0226, train loss = 4.7728776931762695\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0038/0226, train loss = 4.622894763946533\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0039/0226, train loss = 4.479012966156006\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0040/0226, train loss = 4.48411750793457\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0041/0226, train loss = 4.6276068687438965\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0042/0226, train loss = 4.565782070159912\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0043/0226, train loss = 4.689112186431885\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0044/0226, train loss = 4.832080841064453\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0045/0226, train loss = 4.723516941070557\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0046/0226, train loss = 4.8012919425964355\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0047/0226, train loss = 4.640488624572754\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0048/0226, train loss = 4.6564154624938965\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0049/0226, train loss = 4.655832290649414\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0050/0226, train loss = 4.515356540679932\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0051/0226, train loss = 4.750566005706787\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0052/0226, train loss = 4.530016899108887\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0053/0226, train loss = 4.666036605834961\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0054/0226, train loss = 4.666013240814209\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0055/0226, train loss = 4.633485317230225\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0056/0226, train loss = 4.523956775665283\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0057/0226, train loss = 4.665274143218994\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0058/0226, train loss = 4.642331123352051\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0059/0226, train loss = 4.729029655456543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0060/0226, train loss = 4.865610122680664\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0061/0226, train loss = 4.695831775665283\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0062/0226, train loss = 4.681426048278809\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0063/0226, train loss = 4.677123546600342\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0064/0226, train loss = 4.523500442504883\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0065/0226, train loss = 4.644879341125488\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0066/0226, train loss = 4.444182872772217\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0067/0226, train loss = 4.511797904968262\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0068/0226, train loss = 4.646875381469727\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0069/0226, train loss = 4.700347900390625\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0070/0226, train loss = 4.675216197967529\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0071/0226, train loss = 4.55607795715332\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0072/0226, train loss = 4.455325603485107\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0073/0226, train loss = 4.8228440284729\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0074/0226, train loss = 4.692683696746826\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0075/0226, train loss = 4.5916290283203125\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0076/0226, train loss = 4.631428241729736\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0077/0226, train loss = 4.466714382171631\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0078/0226, train loss = 4.509594917297363\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0079/0226, train loss = 4.61467981338501\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0080/0226, train loss = 4.546956539154053\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0081/0226, train loss = 4.656591892242432\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0082/0226, train loss = 4.622453689575195\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0083/0226, train loss = 4.612457752227783\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0084/0226, train loss = 4.609610557556152\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0085/0226, train loss = 4.689746379852295\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0086/0226, train loss = 4.574707508087158\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0087/0226, train loss = 4.458700656890869\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0088/0226, train loss = 4.776636600494385\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0089/0226, train loss = 4.3533854484558105\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0090/0226, train loss = 4.469325065612793\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0091/0226, train loss = 4.624425888061523\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0092/0226, train loss = 4.7285380363464355\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0093/0226, train loss = 4.7295637130737305\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0094/0226, train loss = 4.532427787780762\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0095/0226, train loss = 4.79512882232666\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0096/0226, train loss = 4.507607460021973\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0097/0226, train loss = 4.926969528198242\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0098/0226, train loss = 4.536100387573242\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0099/0226, train loss = 4.369846820831299\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0100/0226, train loss = 4.742205619812012\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0101/0226, train loss = 4.600529670715332\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0102/0226, train loss = 4.6978631019592285\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0103/0226, train loss = 4.571981906890869\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0104/0226, train loss = 4.661892414093018\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0105/0226, train loss = 4.7425360679626465\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0106/0226, train loss = 4.659157752990723\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0107/0226, train loss = 4.676022529602051\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0108/0226, train loss = 4.648171424865723\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0109/0226, train loss = 4.382608890533447\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0110/0226, train loss = 4.459626197814941\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0111/0226, train loss = 4.457187175750732\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0112/0226, train loss = 4.623727321624756\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0113/0226, train loss = 4.776180267333984\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0114/0226, train loss = 4.602856159210205\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0115/0226, train loss = 4.588657379150391\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0116/0226, train loss = 4.673521518707275\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0117/0226, train loss = 4.609443664550781\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0118/0226, train loss = 4.593451499938965\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0119/0226, train loss = 4.5834126472473145\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0120/0226, train loss = 4.496647357940674\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0121/0226, train loss = 4.212480068206787\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0122/0226, train loss = 4.5841169357299805\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0123/0226, train loss = 4.580567836761475\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0124/0226, train loss = 4.703437805175781\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0125/0226, train loss = 4.5580573081970215\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0126/0226, train loss = 4.718271732330322\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0127/0226, train loss = 4.560833930969238\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0128/0226, train loss = 4.360777854919434\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0129/0226, train loss = 4.484382629394531\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0130/0226, train loss = 4.431911945343018\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0131/0226, train loss = 4.618082523345947\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0132/0226, train loss = 4.4945173263549805\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0133/0226, train loss = 4.438316822052002\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0134/0226, train loss = 4.60420036315918\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0135/0226, train loss = 4.687637805938721\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0136/0226, train loss = 4.435336589813232\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0137/0226, train loss = 4.47838020324707\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0138/0226, train loss = 4.431097507476807\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0139/0226, train loss = 4.4799418449401855\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0140/0226, train loss = 4.474976062774658\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0141/0226, train loss = 4.625097274780273\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0142/0226, train loss = 4.609729290008545\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0143/0226, train loss = 4.545406341552734\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0144/0226, train loss = 4.53788423538208\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0145/0226, train loss = 4.601997375488281\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0146/0226, train loss = 4.563766956329346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0147/0226, train loss = 4.598510265350342\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0148/0226, train loss = 4.59454870223999\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0149/0226, train loss = 4.453695297241211\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0150/0226, train loss = 4.5502119064331055\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0151/0226, train loss = 4.471029281616211\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0152/0226, train loss = 4.533938407897949\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0153/0226, train loss = 4.411929130554199\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0154/0226, train loss = 4.509737968444824\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0155/0226, train loss = 4.575893402099609\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0156/0226, train loss = 4.5748467445373535\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0157/0226, train loss = 4.562303066253662\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0158/0226, train loss = 4.18048095703125\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0159/0226, train loss = 4.519052028656006\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0160/0226, train loss = 4.628500938415527\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0161/0226, train loss = 4.452207088470459\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0162/0226, train loss = 4.355223178863525\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0163/0226, train loss = 4.494436264038086\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0164/0226, train loss = 4.337564468383789\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0165/0226, train loss = 4.33811616897583\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0166/0226, train loss = 4.5006608963012695\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0167/0226, train loss = 4.440881252288818\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0168/0226, train loss = 4.484507083892822\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0169/0226, train loss = 4.4609246253967285\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0170/0226, train loss = 4.675014019012451\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0171/0226, train loss = 4.622670650482178\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0172/0226, train loss = 4.224010467529297\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0173/0226, train loss = 4.2650251388549805\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0174/0226, train loss = 4.456125259399414\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0175/0226, train loss = 4.471656799316406\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0176/0226, train loss = 4.544826507568359\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0177/0226, train loss = 4.662412643432617\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0178/0226, train loss = 4.429333686828613\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0179/0226, train loss = 4.267491340637207\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0180/0226, train loss = 4.363814830780029\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0181/0226, train loss = 4.35560941696167\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0182/0226, train loss = 4.03750467300415\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0183/0226, train loss = 4.4101972579956055\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0184/0226, train loss = 4.401384353637695\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0185/0226, train loss = 4.683319091796875\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0186/0226, train loss = 4.501035690307617\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0187/0226, train loss = 4.417476177215576\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0188/0226, train loss = 4.449237823486328\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0189/0226, train loss = 4.4187822341918945\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0190/0226, train loss = 4.4842448234558105\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0191/0226, train loss = 4.405966758728027\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0192/0226, train loss = 4.639589786529541\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0193/0226, train loss = 4.364181995391846\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0194/0226, train loss = 4.476216793060303\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0195/0226, train loss = 4.511241912841797\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0196/0226, train loss = 4.571006774902344\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0197/0226, train loss = 4.504816055297852\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0198/0226, train loss = 4.2467265129089355\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0199/0226, train loss = 4.700553894042969\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0200/0226, train loss = 4.5982666015625\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0201/0226, train loss = 4.442819118499756\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0202/0226, train loss = 4.551777362823486\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0203/0226, train loss = 4.526658535003662\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0204/0226, train loss = 4.563642978668213\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0205/0226, train loss = 4.3396897315979\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0206/0226, train loss = 4.329883575439453\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0207/0226, train loss = 4.362437725067139\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0208/0226, train loss = 4.491188049316406\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0209/0226, train loss = 4.457440376281738\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0210/0226, train loss = 4.480778694152832\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0211/0226, train loss = 4.5232319831848145\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0212/0226, train loss = 4.403810977935791\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0213/0226, train loss = 4.383324146270752\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0214/0226, train loss = 4.317931175231934\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0215/0226, train loss = 4.392446994781494\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0216/0226, train loss = 4.399680137634277\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0217/0226, train loss = 4.390182971954346\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0218/0226, train loss = 4.326245307922363\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0219/0226, train loss = 4.491438865661621\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0220/0226, train loss = 4.503707408905029\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0221/0226, train loss = 4.356307506561279\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0222/0226, train loss = 4.372815132141113\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0223/0226, train loss = 4.391671180725098\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0224/0226, train loss = 4.374462604522705\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0225/0226, train loss = 4.256518363952637\n",
      "Epoch 002/005, valid ppl = 117.95818713098286, batch 0226/0226, train loss = 4.363971710205078\n",
      "Saving the trained model to ../model_dailydialog_rf/model_TS/model_epoch_002.ckpt...\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0001/0226, train loss = 4.216079235076904\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0002/0226, train loss = 4.570042133331299\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0003/0226, train loss = 4.324997901916504\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0004/0226, train loss = 4.470242977142334\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0005/0226, train loss = 4.241279125213623\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0006/0226, train loss = 4.182740211486816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0007/0226, train loss = 4.269045352935791\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0008/0226, train loss = 4.268401622772217\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0009/0226, train loss = 4.319323539733887\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0010/0226, train loss = 4.469695091247559\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0011/0226, train loss = 4.298798084259033\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0012/0226, train loss = 4.149795055389404\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0013/0226, train loss = 4.16184139251709\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0014/0226, train loss = 4.354247570037842\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0015/0226, train loss = 4.483072280883789\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0016/0226, train loss = 4.58226203918457\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0017/0226, train loss = 4.331908226013184\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0018/0226, train loss = 4.36296272277832\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0019/0226, train loss = 4.2899932861328125\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0020/0226, train loss = 4.243616104125977\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0021/0226, train loss = 4.50100564956665\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0022/0226, train loss = 4.274636745452881\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0023/0226, train loss = 4.323075771331787\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0024/0226, train loss = 4.546818733215332\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0025/0226, train loss = 4.61489725112915\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0026/0226, train loss = 4.479669570922852\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0027/0226, train loss = 4.261387348175049\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0028/0226, train loss = 4.524397373199463\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0029/0226, train loss = 4.4998860359191895\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0030/0226, train loss = 4.564019680023193\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0031/0226, train loss = 4.420209884643555\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0032/0226, train loss = 4.460928440093994\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0033/0226, train loss = 4.2764458656311035\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0034/0226, train loss = 4.263221263885498\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0035/0226, train loss = 4.201193809509277\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0036/0226, train loss = 4.490284442901611\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0037/0226, train loss = 4.379177093505859\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0038/0226, train loss = 4.259073734283447\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0039/0226, train loss = 4.439157962799072\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0040/0226, train loss = 4.314310073852539\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0041/0226, train loss = 4.355266094207764\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0042/0226, train loss = 4.409758567810059\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0043/0226, train loss = 4.523897647857666\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0044/0226, train loss = 4.37841796875\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0045/0226, train loss = 4.3402791023254395\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0046/0226, train loss = 4.374632358551025\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0047/0226, train loss = 4.210165977478027\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0048/0226, train loss = 4.278229236602783\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0049/0226, train loss = 4.657963752746582\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0050/0226, train loss = 4.28952693939209\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0051/0226, train loss = 4.295884132385254\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0052/0226, train loss = 4.441267967224121\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0053/0226, train loss = 4.17038106918335\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0054/0226, train loss = 4.50879430770874\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0055/0226, train loss = 4.2679924964904785\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0056/0226, train loss = 4.405150413513184\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0057/0226, train loss = 4.420899868011475\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0058/0226, train loss = 4.492982387542725\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0059/0226, train loss = 4.172690391540527\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0060/0226, train loss = 4.246004581451416\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0061/0226, train loss = 4.088047981262207\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0062/0226, train loss = 4.225437164306641\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0063/0226, train loss = 4.308682441711426\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0064/0226, train loss = 4.310257911682129\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0065/0226, train loss = 4.210868835449219\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0066/0226, train loss = 4.393052577972412\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0067/0226, train loss = 4.115232467651367\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0068/0226, train loss = 4.245282173156738\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0069/0226, train loss = 4.186823844909668\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0070/0226, train loss = 4.113331317901611\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0071/0226, train loss = 4.245467662811279\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0072/0226, train loss = 4.12563943862915\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0073/0226, train loss = 4.484673023223877\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0074/0226, train loss = 4.176808834075928\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0075/0226, train loss = 4.103259086608887\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0076/0226, train loss = 4.251741409301758\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0077/0226, train loss = 4.305323123931885\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0078/0226, train loss = 4.409101486206055\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0079/0226, train loss = 4.246706008911133\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0080/0226, train loss = 4.093799591064453\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0081/0226, train loss = 4.4432196617126465\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0082/0226, train loss = 4.3542280197143555\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0083/0226, train loss = 4.447564601898193\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0084/0226, train loss = 4.365296363830566\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0085/0226, train loss = 4.486639499664307\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0086/0226, train loss = 4.230828285217285\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0087/0226, train loss = 4.309272766113281\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0088/0226, train loss = 4.288999557495117\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0089/0226, train loss = 4.35111141204834\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0090/0226, train loss = 4.343545913696289\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0091/0226, train loss = 4.277484893798828\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0092/0226, train loss = 4.284273624420166\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0093/0226, train loss = 4.26983642578125\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0094/0226, train loss = 4.342354774475098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0095/0226, train loss = 4.371263027191162\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0096/0226, train loss = 4.300204277038574\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0097/0226, train loss = 4.212101459503174\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0098/0226, train loss = 4.426673412322998\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0099/0226, train loss = 4.291828155517578\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0100/0226, train loss = 4.411757946014404\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0101/0226, train loss = 4.454925537109375\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0102/0226, train loss = 4.405818939208984\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0103/0226, train loss = 4.184712886810303\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0104/0226, train loss = 4.3431620597839355\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0105/0226, train loss = 4.160303115844727\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0106/0226, train loss = 4.315304279327393\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0107/0226, train loss = 3.8189268112182617\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0108/0226, train loss = 4.3237433433532715\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0109/0226, train loss = 4.43321418762207\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0110/0226, train loss = 4.295267581939697\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0111/0226, train loss = 4.386445045471191\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0112/0226, train loss = 4.351996898651123\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0113/0226, train loss = 3.868110179901123\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0114/0226, train loss = 4.2055206298828125\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0115/0226, train loss = 4.258036136627197\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0116/0226, train loss = 4.362375736236572\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0117/0226, train loss = 3.9742486476898193\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0118/0226, train loss = 4.3165435791015625\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0119/0226, train loss = 4.355027675628662\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0120/0226, train loss = 4.60039758682251\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0121/0226, train loss = 4.523326396942139\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0122/0226, train loss = 4.309013843536377\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0123/0226, train loss = 4.128566741943359\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0124/0226, train loss = 4.345592021942139\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0125/0226, train loss = 4.356922626495361\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0126/0226, train loss = 4.248626232147217\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0127/0226, train loss = 4.411068916320801\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0128/0226, train loss = 4.156200408935547\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0129/0226, train loss = 3.9694411754608154\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0130/0226, train loss = 4.341238498687744\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0131/0226, train loss = 4.213409423828125\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0132/0226, train loss = 4.261110305786133\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0133/0226, train loss = 4.147256374359131\n",
      "Epoch 003/005, valid ppl = 83.83633605258021, batch 0134/0226, train loss = 4.418385028839111\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_set, valid_set, token2id,id2token = read_data(args.data_path)\n",
    "#     train_set['enc_input'] = train_set['enc_input'][:128,]\n",
    "    \n",
    "    max_uttr_len_enc = train_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = train_set['dec_input'].shape[1]\n",
    "\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "    VAD_loss = np.load(args.VAD_loss_path)\n",
    "    VAD_loss = VAD_loss.reshape(-1,1)\n",
    "    \n",
    "    options = Options(mode = 'TRAIN',\n",
    "                      num_epochs = args.num_epochs,\n",
    "                      batch_size = args.batch_size,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_TS = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_TS.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    if args.restore_epoch > 0:\n",
    "        model_TS.restore(os.path.join(args.restore_path_TS, 'model_TS_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    else:\n",
    "        model_TS.init_tf_vars()\n",
    "    model_TS.train(train_set, VAD,termfreq, VAD_loss,args.save_path_TS, args.restore_epoch, valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Train model P(S|T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert(myset):\n",
    "    enc_input = myset['dec_input'][:,1:]\n",
    "    dec_input =  np.insert(myset['enc_input'], 0, token2id['<go>'], axis=1) # add <go> in the beginning of decoder\n",
    "\n",
    "    target = np.insert(myset['enc_input'], -1, 0, axis=1) \n",
    "    tmp_idx = [np.where(s==0)[0][0] for s in target] \n",
    "    target[np.arange(target.shape[0]),tmp_idx] = token2id['<eos>'] # add <eos> at the end of decoder\n",
    "    \n",
    "    newset = {}\n",
    "    \n",
    "    newset['enc_input'] = enc_input\n",
    "    newset['dec_input'] = dec_input\n",
    "    newset['target'] = target\n",
    "    newset['enc_input_len'] = myset['dec_input_len']\n",
    "    newset['dec_input_len'] = myset['enc_input_len']\n",
    "    return newset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_set, valid_set, token2id,id2token = read_data(args.data_path)\n",
    "    train_set = revert(train_set)\n",
    "    valid_set = revert(valid_set)\n",
    "#     train_set['enc_input'] = train_set['enc_input'][:128,]\n",
    "    \n",
    "    max_uttr_len_enc = train_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = train_set['dec_input'].shape[1]\n",
    "\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "    VAD_loss = np.load(args.VAD_loss_path)\n",
    "    VAD_loss = VAD_loss.reshape(-1,1)\n",
    "    \n",
    "    options = Options(mode = 'TRAIN',\n",
    "                      num_epochs = args.num_epochs,\n",
    "                      batch_size = args.batch_size,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_ST = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_TS.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    if args.restore_epoch > 0:\n",
    "        model_ST.restore(os.path.join(args.restore_path_ST, 'model_TS_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    else:\n",
    "        model_ST.init_tf_vars()\n",
    "    model_ST.train(train_set, VAD,termfreq, VAD_loss,args.save_path_ST, args.restore_epoch, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
