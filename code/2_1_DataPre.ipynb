{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim tutorial:\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set up dictionary for the corpus\n",
    "2. token2id, id2token\n",
    "3. extract questions and answers: question mark; and sentences less than 20 words\n",
    "4. word2vec\n",
    "\n",
    "convert to lower case; but no expansion\n",
    "try not converting lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "import os\n",
    "import chardet   #需要导入这个模块，检测编码格式\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_ = pd.DataFrame(words,columns=['word']).word.value_counts()/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3249.000000\n",
       "mean        0.000308\n",
       "std         0.002668\n",
       "min         0.000003\n",
       "25%         0.000013\n",
       "50%         0.000029\n",
       "75%         0.000072\n",
       "max         0.075629\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3249"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(words))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sent = pos_tag(words)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas = {}\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    lemmas.update({tag[0]:wnl.lemmatize(tag[0], pos=wordnet_pos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word number after lemmatizer\n",
    "len(set(lemmas.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3249 unique tokens: ['!', '$', '%', \"'\", \"'am\"]...)\n"
     ]
    }
   ],
   "source": [
    "corpus_dict = corpora.Dictionary([words])\n",
    "corpus = [corpus_dict.doc2bow(text) for text in [words]]\n",
    "print(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict[0]\n",
    "id2token = corpus_dict.id2token\n",
    "token2id = corpus_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(VAD_dict,open('./pre-data/VAD_dict.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_extend = pd.read_csv('./affect-rich/VAD_extend_clean.csv')\n",
    "VAD_extend.dropna(subset=['Word'],inplace=True)\n",
    "VAD_extend.set_index('Word',inplace=True)\n",
    "\n",
    "lambda_ = 0.1\n",
    "VAD_extend = VAD_extend.clip(lower=3,upper=7) - [5,3,5]\n",
    "VAD_extend = VAD_extend.mul(lambda_)\n",
    "\n",
    "VAD_extend_dict = VAD_extend.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26959"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VAD_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of lemmas in VAD dataframe (after extension)\n",
    "sum([lemma in VAD_extend_dict.keys() for lemma in lemmas.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26959.000000</td>\n",
       "      <td>26959.000000</td>\n",
       "      <td>26959.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.122721</td>\n",
       "      <td>0.018835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107662</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.082669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.065000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.080679</td>\n",
       "      <td>0.165389</td>\n",
       "      <td>0.074333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  V             A             D\n",
       "count  26959.000000  26959.000000  26959.000000\n",
       "mean       0.006111      0.122721      0.018835\n",
       "std        0.107662      0.075985      0.082669\n",
       "min       -0.200000      0.000000     -0.200000\n",
       "25%       -0.065000      0.070000     -0.032000\n",
       "50%        0.016667      0.115000      0.026000\n",
       "75%        0.080679      0.165389      0.074333\n",
       "max        0.200000      0.400000      0.200000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD_extend.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Word2Vec(sentences, min_count=1,size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = []\n",
    "VAD_list = []\n",
    "tf = []\n",
    "for i in range(len(corpus_dict)):\n",
    "    embedding = list(model.wv[id2token[i]])\n",
    "    word_lemma = lemmas[id2token[i]]\n",
    "    vad = list(VAD_extend_dict[word_lemma].values()) if word_lemma in VAD_extend_dict.keys() else [0,0,0]\n",
    "    # after normalization for words outside dictionary the VAD value should be [0,0,0]\n",
    "    word_embeddings.append(np.array(embedding))\n",
    "    VAD_list.append(vad)\n",
    "    tf.append(tf_.loc[id2token[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD = np.array(VAD_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3249.000000</td>\n",
       "      <td>3249.000000</td>\n",
       "      <td>3249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.033663</td>\n",
       "      <td>0.094035</td>\n",
       "      <td>0.033444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.104415</td>\n",
       "      <td>0.094552</td>\n",
       "      <td>0.082482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.110167</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2\n",
       "count  3249.000000  3249.000000  3249.000000\n",
       "mean      0.033663     0.094035     0.033444\n",
       "std       0.104415     0.094552     0.082482\n",
       "min      -0.200000     0.000000    -0.200000\n",
       "25%       0.000000     0.000000     0.000000\n",
       "50%       0.014000     0.076000     0.017000\n",
       "75%       0.110167     0.149000     0.089000\n",
       "max       0.200000     0.400000     0.200000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(VAD).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.array(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.075629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  3249.000000\n",
       "mean      0.000308\n",
       "std       0.002668\n",
       "min       0.000003\n",
       "25%       0.000013\n",
       "50%       0.000029\n",
       "75%       0.000072\n",
       "max       0.075629"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tf).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extract(line,dict_=dict_sub):\n",
    "    encode_type = chardet.detect(line)  \n",
    "    line = line.decode(encode_type['encoding']) #进行相应解码，赋给原标识符（变量）\n",
    "    line = line.lower().replace('<eos>','\\n').split('\\n')\n",
    "#     line = line.replace('<EOS>','\\n').split('\\n')\n",
    "    line = list(filter(None,line))\n",
    "    \n",
    "    # extract easy dialogues\n",
    "    idx = [i for i,x in enumerate(line) if '?' in x]\n",
    "    diag_list = []\n",
    "    for i in idx:\n",
    "        if (i < len(line)-1):\n",
    "            if (len(line[i].split())<20) & (len(line[i+1].split())<20):\n",
    "                diag_list.append(line[i])\n",
    "                diag_list.append('<go> '+line[i+1]+' <eos>')\n",
    "                # diag_list.append(line[i+1])\n",
    "    return diag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in smart_open(os.path.join(self.dirname, fname), 'rb'):\n",
    "                yield from sentence_extract(line)\n",
    "                # return sentence_extract(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/processed_data/OpenSubtitle/'\n",
    "sentences = MyCorpus(file_dir)\n",
    "sentences = [x.lstrip().rstrip() for x in sentences]\n",
    "sentences = list(filter(None,sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token2id = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence_token2id.append([token2id.get(item,item) for item in sentences[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = sentence_token2id[::2]\n",
    "dec_input = [x[:-1] for x in sentence_token2id[1::2]] # '<go>' + \n",
    "target = [x[1:] for x in sentence_token2id[1::2]] # + '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 20 20\n"
     ]
    }
   ],
   "source": [
    "max_uttr_len_enc = max([len(x) for x in enc_input])\n",
    "max_uttr_len_dec = max([len(x) for x in dec_input])\n",
    "max_uttr_len_target = max([len(x) for x in target])\n",
    "print(max_uttr_len_enc,max_uttr_len_dec,max_uttr_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = np.array([i + [0]*(max_uttr_len_enc-len(i)) for i in enc_input])\n",
    "dec_input = np.array([i + [0]*(max_uttr_len_dec-len(i)) for i in dec_input])\n",
    "target = np.array([i + [0]*(max_uttr_len_target-len(i)) for i in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_test, dec_train, dec_test, target_train, target_test = train_test_split(\n",
    "    enc_input, dec_input, target, test_size=0.2, random_state=1)\n",
    "\n",
    "enc_test, enc_val, dec_test, dec_val,target_test,target_val = train_test_split(\n",
    "    enc_test, dec_test, target_test, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "incorporate VAD embedding of words into cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3249, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VAD.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_loss = 1 + delta*np.linalg.norm(VAD,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_loss = V*VAD_loss/sum(VAD_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3323.727584211128"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 + delta*np.linalg.norm(VAD,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3249,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\mu(x_t)$\n",
    "- uniform importance\n",
    "- global importance\n",
    "- local importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform importance\n",
    "mu_ui = np.ones(tf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globale importance\n",
    "a = 0.001\n",
    "mu_gi = a/(a+tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.378590078329"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_gi.max()/mu_gi.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local importance\n",
    "epsilon = 10e-8\n",
    "mu_li = np.log(1/tf+epsilon)/sum(np.log(1/tf+epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.891468104394948"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_li.max()/mu_li.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dec_input: `<go>` + answer\n",
    "target: answer + `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./pre-data/word_embeddings.npy',word_embeddings)\n",
    "np.save('./pre-data/VAD.npy',VAD)\n",
    "np.save('./pre-data/tf.npy',tf)\n",
    "np.save('./pre-data/VAD_loss.npy',VAD_loss)\n",
    "\n",
    "np.save('./pre-data/mu_ui.npy',mu_ui)\n",
    "np.save('./pre-data/mu_gi.npy',mu_gi)\n",
    "np.save('./pre-data/mu_li.npy',mu_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_train]\n",
    "dec_input_len = [sum(x!=0) for x in dec_train]\n",
    "np.save('./pre-data/train/enc_input.npy',enc_train)\n",
    "np.save('./pre-data/train/dec_input.npy',dec_train)\n",
    "np.save('./pre-data/train/target.npy',target_train)\n",
    "np.save('./pre-data/train/enc_input_len.npy',enc_input_len)\n",
    "np.save('./pre-data/train/dec_input_len.npy',dec_input_len)\n",
    "\n",
    "enc_input_len = [sum(x!=0) for x in enc_val]\n",
    "dec_input_len = [sum(x!=0) for x in dec_val]\n",
    "np.save('./pre-data/validation/enc_input.npy',enc_val)\n",
    "np.save('./pre-data/validation/dec_input.npy',dec_val)\n",
    "np.save('./pre-data/validation/target.npy',target_val)\n",
    "np.save('./pre-data/validation/enc_input_len.npy',enc_input_len)\n",
    "np.save('./pre-data/validation/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_test]\n",
    "dec_input_len = [sum(x!=0) for x in dec_test]\n",
    "np.save('./pre-data/test/enc_input.npy',enc_test)\n",
    "np.save('./pre-data/test/dec_input.npy',dec_test)\n",
    "np.save('./pre-data/test/target.npy',target_test)\n",
    "np.save('./pre-data/test/enc_input_len.npy',enc_input_len)\n",
    "np.save('./pre-data/test/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2token,open('./pre-data/id2token.pickle','wb'))\n",
    "pickle.dump(token2id,open('./pre-data/token2id.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('id2token.pickle', 'rb') as file:\n",
    "#     test = pickle.load(file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
