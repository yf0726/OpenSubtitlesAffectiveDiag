{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set up dictionary for the corpus\n",
    "2. token2id, id2token\n",
    "3. extract questions and answers: question mark; and sentences less than 20 words\n",
    "4. word2vec\n",
    "\n",
    "convert to lower case; but no expansion\n",
    "try not converting lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "import os\n",
    "import chardet   #需要导入这个模块，检测编码格式\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "\n",
    "def sentence_extract(line):\n",
    "    try:\n",
    "        encode_type = chardet.detect(line)  \n",
    "        line = line.decode(encode_type['encoding']) #进行相应解码，赋给原标识符（变量）\n",
    "        line = line.lower().split('<eos>')\n",
    "        if len(line)<2:\n",
    "            return []\n",
    "    #     line = line.replace('<EOS>','\\n').split('\\n')\n",
    "        line = list(filter(None,line))[:2]\n",
    "        line = [x.split() for x in line]\n",
    "        diag_list = [line] if (max([len(x) for x in line]) <= 20)&(min([len(x) for x in line]) >= 3) else []\n",
    "    except BaseException:\n",
    "        diag_list = []\n",
    "    return diag_list\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for line in smart_open(self.dirname, 'rb'):\n",
    "            yield from sentence_extract(line)\n",
    "\n",
    "file_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/session_segmentation.txt'\n",
    "corpus = MyCorpus(file_dir)\n",
    "sentences = [item for sublist in corpus for item in sublist]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# def read_lines(dirname):\n",
    "#     line_dict = {}\n",
    "#     with open(dirname, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
    "#         tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#         for line in f:\n",
    "#             line = line.replace('[','').replace(']','')\n",
    "#             tmp = line.split('\\n')[0].split(\" +++$+++ \")\n",
    "#             line_ = nltk.word_tokenize(tmp[-1])\n",
    "# #             line_ = tokenizer.tokenize(tmp[-1].lower())\n",
    "# #             line_ = [x for x in line_ if x not in stop]\n",
    "#             line_dict[tmp[0]] = {'ID1':tmp[1],'MovieID':tmp[2],'Name':tmp[3],'Line':line_}\n",
    "#         return line_dict\n",
    "# movie_lines = read_lines('../datasets/cornell-corpus/movie_lines.txt')\n",
    "# cornell = pd.read_csv('../datasets/cornell-corpus/cleaned.csv')\n",
    "\n",
    "\n",
    "# def sentence_extract(IDs):\n",
    "#     try:\n",
    "#         lines = []\n",
    "#         for ID in IDs:\n",
    "#             line = movie_lines[ID]['Line']\n",
    "#             line = [x.lower().lstrip().rstrip() for x in line]\n",
    "            \n",
    "#             lines.append(line)\n",
    "#         diag_list = [lines] if max([len(x) for x in lines]) > 20 else []\n",
    "            \n",
    "#     except BaseException:\n",
    "#         diag_list = []\n",
    "#     return diag_list\n",
    "\n",
    "# class MyCorpus(object):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    " \n",
    "#     def __iter__(self):\n",
    "#         for i in range(len(self.df)):\n",
    "#             yield from sentence_extract(self.df.LineIDs.loc[i].split(', ')[:2])\n",
    "\n",
    "# corpus = MyCorpus(cornell)\n",
    "# sentences = [item for sublist in corpus for item in sublist]    \n",
    "# sentences = list(filter(None,sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = [item for sublist in sentences for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_ = pd.DataFrame(words_all,columns=['word']).word.value_counts()/len(words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.123100e+04\n",
       "mean     4.710094e-05\n",
       "std      8.895366e-04\n",
       "min      8.586389e-07\n",
       "25%      8.586389e-07\n",
       "50%      2.575917e-06\n",
       "75%      6.869111e-06\n",
       "max      6.593659e-02\n",
       "Name: word, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21231"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(words_all))\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sent = pos_tag(words)\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas = {}\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "    lemmas.update({tag[0]:wnl.lemmatize(tag[0], pos=wordnet_pos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word number after lemmatizer\n",
    "len(set(lemmas.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opst = gensim.models.Word2Vec(iter=1,size=300,min_count=1)  # an empty model, no training yet\n",
    "model_opst.build_vocab(sentences)  # can be a non-repeatable, 1-pass generator\n",
    "\n",
    "model_opst.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz',\n",
    "                                lockf=1.0, # allow further training updates of merged vectors\n",
    "                                binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(718170, 1164634)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opst.train(sentences,total_examples=model_opst.corpus_count,epochs=model_opst.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(21231 unique tokens: ['!', '#', '$', '%', \"'\"]...)\n"
     ]
    }
   ],
   "source": [
    "corpus_dict = corpora.Dictionary([words])\n",
    "corpus = [corpus_dict.doc2bow(text) for text in [words]]\n",
    "print(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict[0]\n",
    "id2token = corpus_dict.id2token\n",
    "token2id = corpus_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(VAD_dict,open('./pre-data/VAD_dict.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_extend = pd.read_csv('./seq2seq_attn/affect-rich/VAD_extend_clean.csv')\n",
    "VAD_extend.dropna(subset=['Word'],inplace=True)\n",
    "VAD_extend.set_index('Word',inplace=True)\n",
    "\n",
    "lambda_ = 0.1\n",
    "VAD_extend = VAD_extend.clip(lower=3,upper=7) - [5,3,5]\n",
    "VAD_extend = VAD_extend.mul(lambda_)\n",
    "\n",
    "VAD_extend_dict = VAD_extend.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26959"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VAD_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12639"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of lemmas in VAD dataframe (after extension)\n",
    "sum([lemma in VAD_extend_dict.keys() for lemma in lemmas.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26959.000000</td>\n",
       "      <td>26959.000000</td>\n",
       "      <td>26959.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.122721</td>\n",
       "      <td>0.018835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107662</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.082669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.065000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.080679</td>\n",
       "      <td>0.165389</td>\n",
       "      <td>0.074333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  V             A             D\n",
       "count  26959.000000  26959.000000  26959.000000\n",
       "mean       0.006111      0.122721      0.018835\n",
       "std        0.107662      0.075985      0.082669\n",
       "min       -0.200000      0.000000     -0.200000\n",
       "25%       -0.065000      0.070000     -0.032000\n",
       "50%        0.016667      0.115000      0.026000\n",
       "75%        0.080679      0.165389      0.074333\n",
       "max        0.200000      0.400000      0.200000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD_extend.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = []\n",
    "VAD_list = []\n",
    "tf = []\n",
    "for i in range(len(corpus_dict)):\n",
    "    embedding = list(model_opst.wv[id2token[i]])\n",
    "    word_lemma = lemmas[id2token[i]]\n",
    "    vad = list(VAD_extend_dict[word_lemma].values()) if word_lemma in VAD_extend_dict.keys() else [0,0,0]\n",
    "    # after normalization for words outside dictionary the VAD value should be [0,0,0]\n",
    "    word_embeddings.append(np.array(embedding))\n",
    "    VAD_list.append(vad)\n",
    "    tf.append(tf_.loc[id2token[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random vector for <go> and <eos>\n",
    "word_embeddings.append(np.random.randn(model_opst.vector_size))\n",
    "VAD_list.append([0,0,0])\n",
    "tf.append(len(sentences)/len(words_all))\n",
    "id2token[len(id2token)] = '<go>'\n",
    "token2id['<go>'] = len(id2token)-1\n",
    "\n",
    "word_embeddings.append(np.random.randn(model_opst.vector_size))\n",
    "VAD_list.append([0,0,0])\n",
    "tf.append(len(sentences)/len(words_all))\n",
    "id2token[len(id2token)] = '<eos>'\n",
    "token2id['<eos>'] = len(id2token)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD = np.array(VAD_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.array(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21233.000000</td>\n",
       "      <td>21233.000000</td>\n",
       "      <td>21233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.011576</td>\n",
       "      <td>0.074317</td>\n",
       "      <td>0.017877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090199</td>\n",
       "      <td>0.091019</td>\n",
       "      <td>0.072521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2\n",
       "count  21233.000000  21233.000000  21233.000000\n",
       "mean       0.011576      0.074317      0.017877\n",
       "std        0.090199      0.091019      0.072521\n",
       "min       -0.200000      0.000000     -0.200000\n",
       "25%        0.000000      0.000000      0.000000\n",
       "50%        0.000000      0.035333      0.000000\n",
       "75%        0.053000      0.130000      0.057000\n",
       "max        0.200000      0.400000      0.200000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(VAD).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/input/'\n",
    "np.save(save_dir+'word_embeddings.npy',word_embeddings)\n",
    "\n",
    "pickle.dump(id2token,open(save_dir+'id2token.pickle','wb'))\n",
    "pickle.dump(token2id,open(save_dir+'token2id.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token2id = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence_token2id.append([token2id.get(item) for item in sentences[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_token2id = [[token2id['<go>']]+x+[token2id['<eos>']] for x in sentence_token2id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = sentence_token2id[::2]\n",
    "dec_input = [[token2id['<go>']]+x for x in sentence_token2id[1::2]] # '<go>' + \n",
    "target = [x+[token2id['<eos>']] for x in sentence_token2id[1::2]] # + '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 21 21\n"
     ]
    }
   ],
   "source": [
    "max_uttr_len_enc = max([len(x) for x in enc_input])\n",
    "max_uttr_len_dec = max([len(x) for x in dec_input])\n",
    "max_uttr_len_target = max([len(x) for x in target])\n",
    "print(max_uttr_len_enc,max_uttr_len_dec,max_uttr_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = np.array([i + [0]*(max_uttr_len_enc-len(i)) for i in enc_input])\n",
    "dec_input = np.array([i + [0]*(max_uttr_len_dec-len(i)) for i in dec_input])\n",
    "target = np.array([i + [0]*(max_uttr_len_target-len(i)) for i in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_test, dec_train, dec_test, target_train, target_test = train_test_split(\n",
    "    enc_input, dec_input, target, test_size=0.3, random_state=1)\n",
    "\n",
    "enc_test, enc_val, dec_test, dec_val,target_test,target_val = train_test_split(\n",
    "    enc_test, dec_test, target_test, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66207, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46344, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9931, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "incorporate VAD embedding of words into cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = VAD.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_loss = 1 + delta*np.linalg.norm(VAD,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_loss = V*VAD_loss/sum(VAD_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 + delta*np.linalg.norm(VAD,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VAD_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\\mu(x_t)$\n",
    "- uniform importance\n",
    "- global importance\n",
    "- local importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform importance\n",
    "mu_ui = np.ones(tf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globale importance\n",
    "a = 0.001\n",
    "mu_gi = a/(a+tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_gi.max()/mu_gi.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# local importance\n",
    "epsilon = 10e-8\n",
    "mu_li = np.log(1/tf+epsilon)/sum(np.log(1/tf+epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_li.max()/mu_li.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dec_input: `<go>` + answer\n",
    "target: answer + `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_dir+'VAD.npy',VAD)\n",
    "np.save(save_dir+'tf.npy',tf)\n",
    "np.save(save_dir+'VAD_loss.npy',VAD_loss)\n",
    "\n",
    "np.save(save_dir+'mu_ui.npy',mu_ui)\n",
    "np.save(save_dir+'mu_gi.npy',mu_gi)\n",
    "np.save(save_dir+'mu_li.npy',mu_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_train]\n",
    "dec_input_len = [sum(x!=0) for x in dec_train]\n",
    "np.save(save_dir+'/train/enc_input.npy',enc_train)\n",
    "np.save(save_dir+'/train/dec_input.npy',dec_train)\n",
    "np.save(save_dir+'/train/target.npy',target_train)\n",
    "np.save(save_dir+'/train/enc_input_len.npy',enc_input_len)\n",
    "np.save(save_dir+'/train/dec_input_len.npy',dec_input_len)\n",
    "\n",
    "enc_input_len = [sum(x!=0) for x in enc_val]\n",
    "dec_input_len = [sum(x!=0) for x in dec_val]\n",
    "np.save(save_dir+'/validation/enc_input.npy',enc_val)\n",
    "np.save(save_dir+'/validation/dec_input.npy',dec_val)\n",
    "np.save(save_dir+'/validation/target.npy',target_val)\n",
    "np.save(save_dir+'/validation/enc_input_len.npy',enc_input_len)\n",
    "np.save(save_dir+'/validation/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_test]\n",
    "dec_input_len = [sum(x!=0) for x in dec_test]\n",
    "np.save(save_dir+'/test/enc_input.npy',enc_test)\n",
    "np.save(save_dir+'/test/dec_input.npy',dec_test)\n",
    "np.save(save_dir+'/test/target.npy',target_test)\n",
    "np.save(save_dir+'/test/enc_input_len.npy',enc_input_len)\n",
    "np.save(save_dir+'/test/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
