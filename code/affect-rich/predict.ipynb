{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from model import Options, Seq2SeqAttn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beautifultable import BeautifulTable\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the command line arguments.\n",
    "save_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/input/'\n",
    "output_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type = str, default = save_dir,\n",
    "                    help = 'the directory to the data')\n",
    "\n",
    "parser.add_argument('--word_embeddings_path', type = str, default = save_dir+'word_embeddings.npy',\n",
    "                    help = 'the directory to the pre-trained word embeddings')\n",
    "parser.add_argument('--VAD_path', type = str, default = save_dir+'VAD.npy',\n",
    "                    help = 'the directory to VAD')\n",
    "parser.add_argument('--tf_path', type = str, default = save_dir+'tf.npy',\n",
    "                    help = 'the directory to term frequency')\n",
    "parser.add_argument('--VAD_loss_path', type = str, default = save_dir+'VAD_loss.npy',\n",
    "                    help = 'the directory to VAD loss for each word')\n",
    "parser.add_argument('--ti_path', type = str, default = save_dir+'mu_li.npy',\n",
    "                    help = 'the directory to term importance')\n",
    "\n",
    "parser.add_argument('--num_epochs', type = int, default = 3,\n",
    "                    help = 'the number of epochs to train the data')\n",
    "parser.add_argument('--batch_size', type = int, default = 64,\n",
    "                    help = 'the batch size')\n",
    "parser.add_argument('--learning_rate', type = float, default = 0.001,\n",
    "                    help = 'the learning rate')\n",
    "parser.add_argument('--beam_width', type = int, default = 32,\n",
    "                    help = 'the beam width when decoding')\n",
    "parser.add_argument('--word_embed_size', type = int, default = 300,\n",
    "                    help = 'the size of word embeddings')\n",
    "parser.add_argument('--n_hidden_units_enc', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of encoder')\n",
    "parser.add_argument('--n_hidden_units_dec', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of decoder')\n",
    "parser.add_argument('--attn_depth', type = int, default = 128,\n",
    "                    help = 'attention depth')\n",
    "\n",
    "parser.add_argument('--restore_path_TS', type = str, default = output_dir+'model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_TS', type = str, default = output_dir+'/model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_path_ST', type = str, default = output_dir+'model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_ST', type = str, default = output_dir+'/model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_epoch', type = int, default = 3,\n",
    "                    help = 'the epoch to restore')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        \n",
    "        # to check if or not to complete the last batch\n",
    "        idx = np.arange(my_set['dec_input'].shape[0])\n",
    "        left_samples = idx[-1]%args.batch_size\n",
    "        if left_samples:\n",
    "            last_batch_idx = np.random.randint(0,idx[-1]-left_samples,size = args.batch_size - left_samples - 1)\n",
    "            idx = np.concatenate([idx,last_batch_idx])\n",
    "            \n",
    "            my_set['enc_input'] = my_set['enc_input'][idx]\n",
    "            my_set['dec_input'] = my_set['dec_input'][idx]\n",
    "            my_set['target'] = my_set['target'][idx]\n",
    "            my_set['enc_input_len'] = my_set['enc_input_len'][idx]\n",
    "            my_set['dec_input_len'] = my_set['dec_input_len'][idx]\n",
    "        return my_set\n",
    "    test_set = load_np_files(os.path.join(data_path, 'test'))\n",
    "    # dictionary index of words\n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "    with open(os.path.join(data_path, 'id2token.pickle'), 'rb') as file:\n",
    "        id2token = pickle.load(file)\n",
    "    return test_set, token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, uttr_len, id2token): # ?\n",
    "    tokens = []\n",
    "    if uttr_len is not None:\n",
    "        for i in range(uttr_len):\n",
    "            if id2token[ids[i]] != '<eos>' and id2token[ids[i]] != '<go>':\n",
    "                tokens.append(id2token[ids[i]])\n",
    "    else:\n",
    "        i = 0\n",
    "        while i < len(ids) and id2token[ids[i]] != '<eos>':\n",
    "            tokens.append(id2token[ids[i]])\n",
    "            i += 1\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert(myset):\n",
    "    enc_input = myset['dec_input'][:,1:]\n",
    "    dec_input = np.insert(myset['enc_input'], 0, token2id['<go>'], axis=1) # add <go> in the beginning of encoder\n",
    "\n",
    "    target = np.insert(myset['enc_input'], -1, 0, axis=1) \n",
    "    tmp_idx = [np.where(s==0)[0][0] for s in target] \n",
    "    target[np.arange(target.shape[0]),tmp_idx] = token2id['<eos>'] # add <eos> at the end of encoder\n",
    "    \n",
    "    newset = {}\n",
    "    \n",
    "    newset['enc_input'] = enc_input\n",
    "    newset['dec_input'] = dec_input\n",
    "    newset['target'] = target\n",
    "    newset['enc_input_len'] = myset['dec_input_len']\n",
    "    newset['dec_input_len'] = myset['enc_input_len']\n",
    "    return newset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given enc_input predict prediction P(T|S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, token2id, id2token = read_data(args.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9984"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set['enc_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "Restoring a pre-trained model from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_TS/model_epoch_003.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_TS/model_epoch_003.ckpt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     test_set, token2id, id2token = read_data(args.data_path)\n",
    "    max_uttr_len_enc = test_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = test_set['dec_input'].shape[1]\n",
    "    \n",
    "    test_set['enc_input'] = test_set['enc_input'][:5*args.batch_size]\n",
    "\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "    VAD_loss = np.load(args.VAD_loss_path)\n",
    "    VAD_loss = VAD_loss.reshape(-1,1)\n",
    "\n",
    "    options = Options(mode = 'PREDICT',\n",
    "                      VAD_mode = 'FALSE',\n",
    "                      num_epochs = args.num_epochs,\n",
    "                      batch_size = args.batch_size,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_TS = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_TS.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    model_TS.restore(os.path.join(args.restore_path_TS, 'model_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    prediction_TS,probability_TS = model_TS.predict(test_set['enc_input'], test_set['enc_input_len'],VAD,termfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 21, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.4579206,  -6.605519 ,  -6.68667  , ..., -18.483994 ,\n",
       "        -18.982075 , -25.699299 ],\n",
       "       [ -5.67196  ,  -7.569267 ,  -7.5700526, ..., -21.53505  ,\n",
       "        -21.718372 , -22.024775 ],\n",
       "       [ -6.3648877,  -7.661137 ,  -7.7334   , ..., -18.038395 ,\n",
       "        -18.908665 , -28.184574 ],\n",
       "       ...,\n",
       "       [ -6.554825 ,  -7.3182216,  -7.3226833, ..., -19.175753 ,\n",
       "        -23.30402  , -27.547388 ],\n",
       "       [ -6.267722 ,  -7.088844 ,  -7.1873302, ..., -26.72459  ,\n",
       "        -29.60926  , -31.220829 ],\n",
       "       [ -6.704087 ,  -6.955397 ,  -7.663645 , ..., -18.191755 ,\n",
       "        -18.73783  , -24.330801 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/prediction_TS.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction_TS, f)\n",
    "    \n",
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/probability_TS.pickle', 'wb') as f:\n",
    "    pickle.dump(probability_TS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "given prediction predict enc_input P(T|S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pre-data/test/prediction_TS_noVAD.pickle', 'rb') as file:\n",
    "    prediction_TS = pickle.load(file)\n",
    "    \n",
    "with open('../pre-data/test/probability_TS_noVAD.pickle', 'rb') as file:\n",
    "    probability_TS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every i in range(prediction_TS.shape[0])\n",
    "# enc_input[i:i+args.beam_width] is the prediction of top args.beam_width of one given source from model_TS\n",
    "enc_input = prediction_TS[0,:,:].T\n",
    "for i in range(prediction_TS.shape[0]-1):\n",
    "    enc_input = np.concatenate((enc_input,prediction_TS[i+1,:,:].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 21, 32) (10240, 21)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_TS.shape,enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set = {}\n",
    "new_test_set['enc_input'] = enc_input\n",
    "\n",
    "multi_idx = np.tile(np.arange(test_set['enc_input'].shape[0]).T,(args.beam_width,1)).T.ravel()\n",
    "\n",
    "new_test_set['dec_input'] = np.insert(test_set['enc_input'][multi_idx], 0, token2id['<go>'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.insert(test_set['enc_input'], -1, 0, axis=1) \n",
    "tmp_idx = [np.where(s==0)[0][0] for s in target] \n",
    "target[np.arange(target.shape[0]),tmp_idx] = token2id['<eos>'] # add <eos> at the end of encoder\n",
    "\n",
    "new_test_set['target'] = target[multi_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set['enc_input_len'] = (enc_input.shape[1]*np.ones(enc_input.shape[0])).astype(int)\n",
    "_,idx = np.unique(np.argwhere(enc_input==token2id['<eos>'])[:,0],return_index=True)\n",
    "# multi <eos> in one sentence, so find the first <eos> in each row\n",
    "# for those predictions without <eos> the length is max_len\n",
    "new_test_set['enc_input_len'][np.argwhere(enc_input==token2id['<eos>'])[idx,0]] = np.argwhere(enc_input==token2id['<eos>'])[idx,1]\n",
    "\n",
    "new_test_set['dec_input_len'] = np.tile(test_set['enc_input_len'],(args.beam_width,1)).T.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "Restoring a pre-trained model from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_ST/model_epoch_003.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_ST/model_epoch_003.ckpt\n",
      "Start to train the model...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     test_set, token2id, id2token = read_data(args.data_path)\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "\n",
    "    max_uttr_len_enc = new_test_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = new_test_set['dec_input'].shape[1]\n",
    "    \n",
    "    options = Options(mode = 'POST_PREDICT',\n",
    "                      VAD_mode = 'FALSE',\n",
    "                      num_epochs = 1,\n",
    "                      batch_size = 1,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_ST = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_ST.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    model_ST.restore(os.path.join(args.restore_path_ST, 'model_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    probability_ST = model_ST.post_predict(new_test_set, VAD,termfreq)\n",
    "    probability_ST = probability_ST.reshape(-1,args.beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/probability_ST.pickle', 'wb') as f:\n",
    "    pickle.dump(probability_ST, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_VAD(sentence,VAD,s_len):\n",
    "    vad = 0\n",
    "    for i in range(s_len):\n",
    "        vad += sum(abs(VAD[sentence[i]]))\n",
    "    return vad/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMI_bidi(pred_TS,prob_TS,prob_ST,VAD,id2token):\n",
    "    \"\"\"\n",
    "    pred_TS: [num_sentence, max_uttr_len_dec, beam_width]\n",
    "    prob_TS, prob_ST: [num_sentence, beam_width]\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['label_num','label','target',\"prediction\", \"prob_TS\", \"prob_ST\",'ABS_VAD'])\n",
    "    \n",
    "    for sentence_num in range(pred_TS.shape[0]):\n",
    "        bias = sentence_num*args.beam_width\n",
    "        target = ids_to_sentence(test_set['dec_input'][sentence_num], test_set['dec_input_len'][sentence_num], id2token)\n",
    "        for i in range(args.beam_width):\n",
    "            label = ids_to_sentence(new_test_set['dec_input'][bias+i], new_test_set['dec_input_len'][bias+i]+1, id2token)\n",
    "            pred_s = ids_to_sentence(pred_TS[sentence_num,:,i], new_test_set['enc_input_len'][bias+i], id2token)\n",
    "            vad = sentence_VAD(pred_TS[sentence_num,:,i], VAD, new_test_set['enc_input_len'][bias+i])\n",
    "            df.loc[bias+i] = list([sentence_num,label,target,pred_s,prob_TS[sentence_num,i],\n",
    "                              prob_ST[sentence_num,i],vad])    \n",
    "#     print(table)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_VAD = MMI_bidi(prediction_TS,probability_TS,probability_ST,VAD,id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAD.to_csv('MMI_VAD.csv',index=False)\n",
    "df = pd.read_csv('MMI.csv')\n",
    "df_VAD = pd.read_csv('MMI_VAD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "outputs1 = []\n",
    "outputs2 = []\n",
    "\n",
    "for i in range(int(len(df)/32)):\n",
    "    input_ = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_TS',ascending=False).head(10).iloc[0].label\n",
    "    target = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_TS',ascending=False).head(10).iloc[0].target\n",
    "    TS_response = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_TS',ascending=False).head(10).iloc[0].prediction\n",
    "    ST_response = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_ST',ascending=False).head(10).sort_values(by=\"ABS_VAD\",ascending=False).iloc[0].prediction\n",
    "    \n",
    "    inputs.append(input_)\n",
    "    targets.append(target)\n",
    "    outputs1.append(TS_response)\n",
    "    outputs2.append(ST_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Target</th>\n",
       "      <th>Basic</th>\n",
       "      <th>MMI_VAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what 's your name ?</td>\n",
       "      <td>elena , but my friends call me yo-yo .</td>\n",
       "      <td>my name .</td>\n",
       "      <td>i do n't know . i was just looking for my friend .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why did i agree to this ?</td>\n",
       "      <td>it 's like you said , there 's no roads in or out of here .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know , but i did n't know why i was talking about it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>done . what do you call yourself ?</td>\n",
       "      <td>winston will have to do .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i do n't understand .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>care to help ?</td>\n",
       "      <td>uh , why do n't i go with you , since i know the world and all ?</td>\n",
       "      <td>of course .</td>\n",
       "      <td>i do n't know . i do n't know , sweetheart .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do you want to see your daughter alive ?</td>\n",
       "      <td>do n't hurt her .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't think .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>someone other than fitz ?</td>\n",
       "      <td>oh , well , of course , fitz , but he has been rather distracted lately .</td>\n",
       "      <td>yeah , of course .</td>\n",
       "      <td>well , of course he does , but he does n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how far is the renaissance hotel ?</td>\n",
       "      <td>it 's too far to walk .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . it 's all .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>so you think he 's doing it again ?</td>\n",
       "      <td>uh , well , uh , uh ...</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>no , i do n't know , but i do n't know , but i do n't know , but i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he caught it for me ?</td>\n",
       "      <td>you know , in that moment , it all seemed possible .</td>\n",
       "      <td>he stabs it .</td>\n",
       "      <td>i do n't know . i do n't know what you 're talking about .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>turn around ? what are you gonna fuck me first ?</td>\n",
       "      <td>no , you 're the fuck up , you fucked up royally .</td>\n",
       "      <td>i 'm sorry .</td>\n",
       "      <td>i do n't know . i do n't know . i do n't want you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>you mean miguel 's ?</td>\n",
       "      <td>yeah . that 's how family works .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>i do n't know . i 'm sorry .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are you gonna miss me ?</td>\n",
       "      <td>lykke : do you want me to stay ?</td>\n",
       "      <td>i am .</td>\n",
       "      <td>i do n't know , sweetheart .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what 's your count ?</td>\n",
       "      <td>what you see is what i got .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i do n't know . i do n't know . i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>where are you taking me ?</td>\n",
       "      <td>look at that . i did n't even need the whole day .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't want to talk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>what about the possibility of error ?</td>\n",
       "      <td>then what could be more noble than sacrificing ourselves for the greater glory of the cause ?</td>\n",
       "      <td>not yet .</td>\n",
       "      <td>i do n't know , but i can n't tell you .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>are you sure she 's not ? because i seem to have a very intimate recollection of one night ...</td>\n",
       "      <td>all right , then .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>i do n't know , sweetheart .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>say what again ?</td>\n",
       "      <td>i do n't wanna do this anymore .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . that 's right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what do you mean bad choices ?</td>\n",
       "      <td>i used to be a hacktivist .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't understand .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what are you ?</td>\n",
       "      <td>i am lumiere</td>\n",
       "      <td>what are you doing ?</td>\n",
       "      <td>i 'm sorry . i 'm fine . i 'm not sure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>and your mother-in-law 's going to examine you ?</td>\n",
       "      <td>'can i see a picture of the lucky man ? '</td>\n",
       "      <td>of course .</td>\n",
       "      <td>that 's right . it 's all right . it 's all right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what do you want me to do ? i 'll do anything .</td>\n",
       "      <td>out . now</td>\n",
       "      <td>i do .</td>\n",
       "      <td>i do n't know . i do n't want you to be .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>you 're saying that all my parishioners are drug dealers ?</td>\n",
       "      <td>no ... there 's no way to know . it 's probably just a few of them .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how come they never make quesadillas with sausage ?</td>\n",
       "      <td>and call it a pizza-dilla ... what you think ?</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>shall we consider ' operation sea-sight a success ?</td>\n",
       "      <td>move aside ... move ...</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i can n't .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>and they believed her ?</td>\n",
       "      <td>right . of course they did .</td>\n",
       "      <td>yes , of course .</td>\n",
       "      <td>i do n't know . i do n't know . that 's right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>have you got a summary of the primary survey ?</td>\n",
       "      <td>we 've got an &lt;num&gt;-year-old with penetrating neck trauma and asystolic arrest .</td>\n",
       "      <td>not yet .</td>\n",
       "      <td>i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>did you get their information ? where can we find them ?</td>\n",
       "      <td>i do n't have that .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i can n't . i can n't . i do n't .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>alright , where does that lead you ?</td>\n",
       "      <td>mikey used to always go to an arcade after school .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>recompense . what ?</td>\n",
       "      <td>i can give you money .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . it 's just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>do you have any suspects ?</td>\n",
       "      <td>yeah , we got one in custody .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>i do n't know . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>you wanna reason with this ?</td>\n",
       "      <td>yeah , maybe use words . you ever try that ?</td>\n",
       "      <td>i am .</td>\n",
       "      <td>i do n't understand .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>did you smell his breath ?</td>\n",
       "      <td>i 'd put money he was sober , ma 'am .</td>\n",
       "      <td>i did .</td>\n",
       "      <td>i mean , i did n't .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>who did you say procured this intelligence ?</td>\n",
       "      <td>in camp , she went by mary smith .</td>\n",
       "      <td>i did .</td>\n",
       "      <td>i do n't know . it 's all right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>for women , it 's would you watch trashy reality shows ?</td>\n",
       "      <td>take , for instance , michelle obama .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>no , i 'm sorry . i 'm sorry . i 'm not sure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>can i put mine down ? they 're really heavy .</td>\n",
       "      <td>well , in a drifting competition , you have what 's called clipping points .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>no , i can n't . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>and that 's the serpents ?</td>\n",
       "      <td>when the time comes , they 'll step up .</td>\n",
       "      <td>yes , sir .</td>\n",
       "      <td>i do n't know . i do n't know . that 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>hit ' em long and straight ?</td>\n",
       "      <td>all right . thanks again for the beer .</td>\n",
       "      <td>all right .</td>\n",
       "      <td>that 's right . it 's all right . it 's all right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>you wanna try something ?</td>\n",
       "      <td>come at me . let 's go</td>\n",
       "      <td>of course .</td>\n",
       "      <td>i do n't know . i just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>you need to move on , eh ?</td>\n",
       "      <td>what you did was unforgivable .</td>\n",
       "      <td>i do .</td>\n",
       "      <td>i do n't want to be honest .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>so this assignment is sort of like being put out to pasture ?</td>\n",
       "      <td>that 's not the metaphor i 'd use , no .</td>\n",
       "      <td>yes , sir .</td>\n",
       "      <td>i 'm not sure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>great dish . packing that wonton with that amount of lobster and not having them burst ? amazing .</td>\n",
       "      <td>good job .</td>\n",
       "      <td>of course .</td>\n",
       "      <td>all right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>the only mystery to me is why all the damn questions ?</td>\n",
       "      <td>i mean , you know who did it , for god 's sake .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>what do you want from me , master ?</td>\n",
       "      <td>i want you to sit down .</td>\n",
       "      <td>i need your help .</td>\n",
       "      <td>i want you to be happy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>have you no fucking respect , dan anderssen ?</td>\n",
       "      <td>this is hildur 's place</td>\n",
       "      <td>of course .</td>\n",
       "      <td>no , i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>and what if it does n't work ?</td>\n",
       "      <td>the most likely risk is it does n't do anything at all .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>well , it does n't matter . it 's all right . it 's all right .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>star : you know what amazes me ?</td>\n",
       "      <td>alex thinks she 's all that because a bunch of drunks at a barbecue told her she is .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know what i want .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>what kind of question is that ?</td>\n",
       "      <td>be quiet , anna .</td>\n",
       "      <td>oh , my god .</td>\n",
       "      <td>i do n't know . i do n't want to be .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>you know ? i think maybe erin should run a dna test on you .</td>\n",
       "      <td>i 'm going to say something that i probably have n't said in &lt;num&gt; years .</td>\n",
       "      <td>i do .</td>\n",
       "      <td>i do n't know . i do n't want to talk about it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>how do you do that ?</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't understand .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>look , i do n't want to work at a lab at mit , okay ?</td>\n",
       "      <td>i 'm sorry . i 'm sorry .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know anything .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>how are you ? i am nuomin</td>\n",
       "      <td>nice to meet you</td>\n",
       "      <td>oh , my god .</td>\n",
       "      <td>i do n't know . i do n't know . i 'm just saying .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>louis , are you kidding ? this is n't dubai .</td>\n",
       "      <td>he 's not gonna be able to do that .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i 'm not sure .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>? the baudelaires ' new guardian is wracked with fear and panic ?</td>\n",
       "      <td>? they end up on a boat that might as well be the titanic ?</td>\n",
       "      <td>not yet .</td>\n",
       "      <td>i do n't know . i do n't think so .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>you think i care what happens to me ?</td>\n",
       "      <td>i am sick of watching you destroy people 's lives and get away with it .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>do you know where the bathroom is ?</td>\n",
       "      <td>â™ª but it 's a state of undress</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i do .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>what do you think ?</td>\n",
       "      <td>melanie throws you in the lake . charlie confesses and luc attacks me .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . i do n't know anything .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>can i just say one thing ?</td>\n",
       "      <td>you never leave it at one thing .</td>\n",
       "      <td>oh , yeah .</td>\n",
       "      <td>i can n't .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>how much you weigh ?</td>\n",
       "      <td>&lt;num&gt; , give or take .</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . it 's a good man .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>well , let 's start with ... what is it that you can do to protect yourself ?</td>\n",
       "      <td>what is anything you 're doing that might provoke kids ?</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know what you 're talking about . it 's just a few more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>playing with the gardener and that awful girl who 's always following her around . are you staying long ?</td>\n",
       "      <td>would you like a drink ? a juice ?</td>\n",
       "      <td>i do n't know .</td>\n",
       "      <td>i do n't know . i do n't know . it 's just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         Input  \\\n",
       "0    what 's your name ?                                                                                         \n",
       "1    why did i agree to this ?                                                                                   \n",
       "2    done . what do you call yourself ?                                                                          \n",
       "3    care to help ?                                                                                              \n",
       "4    do you want to see your daughter alive ?                                                                    \n",
       "5    someone other than fitz ?                                                                                   \n",
       "6    how far is the renaissance hotel ?                                                                          \n",
       "7    so you think he 's doing it again ?                                                                         \n",
       "8    he caught it for me ?                                                                                       \n",
       "9    turn around ? what are you gonna fuck me first ?                                                            \n",
       "10   you mean miguel 's ?                                                                                        \n",
       "11   are you gonna miss me ?                                                                                     \n",
       "12   what 's your count ?                                                                                        \n",
       "13   where are you taking me ?                                                                                   \n",
       "14   what about the possibility of error ?                                                                       \n",
       "15   are you sure she 's not ? because i seem to have a very intimate recollection of one night ...              \n",
       "16   say what again ?                                                                                            \n",
       "17   what do you mean bad choices ?                                                                              \n",
       "18   what are you ?                                                                                              \n",
       "19   and your mother-in-law 's going to examine you ?                                                            \n",
       "20   what do you want me to do ? i 'll do anything .                                                             \n",
       "21   you 're saying that all my parishioners are drug dealers ?                                                  \n",
       "22   how come they never make quesadillas with sausage ?                                                         \n",
       "23   shall we consider ' operation sea-sight a success ?                                                         \n",
       "24   and they believed her ?                                                                                     \n",
       "25   have you got a summary of the primary survey ?                                                              \n",
       "26   did you get their information ? where can we find them ?                                                    \n",
       "27   alright , where does that lead you ?                                                                        \n",
       "28   recompense . what ?                                                                                         \n",
       "29   do you have any suspects ?                                                                                  \n",
       "..                          ...                                                                                  \n",
       "290  you wanna reason with this ?                                                                                \n",
       "291  did you smell his breath ?                                                                                  \n",
       "292  who did you say procured this intelligence ?                                                                \n",
       "293  for women , it 's would you watch trashy reality shows ?                                                    \n",
       "294  can i put mine down ? they 're really heavy .                                                               \n",
       "295  and that 's the serpents ?                                                                                  \n",
       "296  hit ' em long and straight ?                                                                                \n",
       "297  you wanna try something ?                                                                                   \n",
       "298  you need to move on , eh ?                                                                                  \n",
       "299  so this assignment is sort of like being put out to pasture ?                                               \n",
       "300  great dish . packing that wonton with that amount of lobster and not having them burst ? amazing .          \n",
       "301  the only mystery to me is why all the damn questions ?                                                      \n",
       "302  what do you want from me , master ?                                                                         \n",
       "303  have you no fucking respect , dan anderssen ?                                                               \n",
       "304  and what if it does n't work ?                                                                              \n",
       "305  star : you know what amazes me ?                                                                            \n",
       "306  what kind of question is that ?                                                                             \n",
       "307  you know ? i think maybe erin should run a dna test on you .                                                \n",
       "308  how do you do that ?                                                                                        \n",
       "309  look , i do n't want to work at a lab at mit , okay ?                                                       \n",
       "310  how are you ? i am nuomin                                                                                   \n",
       "311  louis , are you kidding ? this is n't dubai .                                                               \n",
       "312  ? the baudelaires ' new guardian is wracked with fear and panic ?                                           \n",
       "313  you think i care what happens to me ?                                                                       \n",
       "314  do you know where the bathroom is ?                                                                         \n",
       "315  what do you think ?                                                                                         \n",
       "316  can i just say one thing ?                                                                                  \n",
       "317  how much you weigh ?                                                                                        \n",
       "318  well , let 's start with ... what is it that you can do to protect yourself ?                               \n",
       "319  playing with the gardener and that awful girl who 's always following her around . are you staying long ?   \n",
       "\n",
       "                                                                                            Target  \\\n",
       "0    elena , but my friends call me yo-yo .                                                          \n",
       "1    it 's like you said , there 's no roads in or out of here .                                     \n",
       "2    winston will have to do .                                                                       \n",
       "3    uh , why do n't i go with you , since i know the world and all ?                                \n",
       "4    do n't hurt her .                                                                               \n",
       "5    oh , well , of course , fitz , but he has been rather distracted lately .                       \n",
       "6    it 's too far to walk .                                                                         \n",
       "7    uh , well , uh , uh ...                                                                         \n",
       "8    you know , in that moment , it all seemed possible .                                            \n",
       "9    no , you 're the fuck up , you fucked up royally .                                              \n",
       "10   yeah . that 's how family works .                                                               \n",
       "11   lykke : do you want me to stay ?                                                                \n",
       "12   what you see is what i got .                                                                    \n",
       "13   look at that . i did n't even need the whole day .                                              \n",
       "14   then what could be more noble than sacrificing ourselves for the greater glory of the cause ?   \n",
       "15   all right , then .                                                                              \n",
       "16   i do n't wanna do this anymore .                                                                \n",
       "17   i used to be a hacktivist .                                                                     \n",
       "18   i am lumiere                                                                                    \n",
       "19   'can i see a picture of the lucky man ? '                                                       \n",
       "20   out . now                                                                                       \n",
       "21   no ... there 's no way to know . it 's probably just a few of them .                            \n",
       "22   and call it a pizza-dilla ... what you think ?                                                  \n",
       "23   move aside ... move ...                                                                         \n",
       "24   right . of course they did .                                                                    \n",
       "25   we 've got an <num>-year-old with penetrating neck trauma and asystolic arrest .                \n",
       "26   i do n't have that .                                                                            \n",
       "27   mikey used to always go to an arcade after school .                                             \n",
       "28   i can give you money .                                                                          \n",
       "29   yeah , we got one in custody .                                                                  \n",
       "..                              ...                                                                  \n",
       "290  yeah , maybe use words . you ever try that ?                                                    \n",
       "291  i 'd put money he was sober , ma 'am .                                                          \n",
       "292  in camp , she went by mary smith .                                                              \n",
       "293  take , for instance , michelle obama .                                                          \n",
       "294  well , in a drifting competition , you have what 's called clipping points .                    \n",
       "295  when the time comes , they 'll step up .                                                        \n",
       "296  all right . thanks again for the beer .                                                         \n",
       "297  come at me . let 's go                                                                          \n",
       "298  what you did was unforgivable .                                                                 \n",
       "299  that 's not the metaphor i 'd use , no .                                                        \n",
       "300  good job .                                                                                      \n",
       "301  i mean , you know who did it , for god 's sake .                                                \n",
       "302  i want you to sit down .                                                                        \n",
       "303  this is hildur 's place                                                                         \n",
       "304  the most likely risk is it does n't do anything at all .                                        \n",
       "305  alex thinks she 's all that because a bunch of drunks at a barbecue told her she is .           \n",
       "306  be quiet , anna .                                                                               \n",
       "307  i 'm going to say something that i probably have n't said in <num> years .                      \n",
       "308  i do n't know .                                                                                 \n",
       "309  i 'm sorry . i 'm sorry .                                                                       \n",
       "310  nice to meet you                                                                                \n",
       "311  he 's not gonna be able to do that .                                                            \n",
       "312  ? they end up on a boat that might as well be the titanic ?                                     \n",
       "313  i am sick of watching you destroy people 's lives and get away with it .                        \n",
       "314  â™ª but it 's a state of undress                                                                \n",
       "315  melanie throws you in the lake . charlie confesses and luc attacks me .                         \n",
       "316  you never leave it at one thing .                                                               \n",
       "317  <num> , give or take .                                                                          \n",
       "318  what is anything you 're doing that might provoke kids ?                                        \n",
       "319  would you like a drink ? a juice ?                                                              \n",
       "\n",
       "                    Basic  \\\n",
       "0    my name .              \n",
       "1    i do n't know .        \n",
       "2    i do n't know .        \n",
       "3    of course .            \n",
       "4    i do n't know .        \n",
       "5    yeah , of course .     \n",
       "6    i do n't know .        \n",
       "7    i do n't know .        \n",
       "8    he stabs it .          \n",
       "9    i 'm sorry .           \n",
       "10   of course .            \n",
       "11   i am .                 \n",
       "12   i do n't know .        \n",
       "13   i do n't know .        \n",
       "14   not yet .              \n",
       "15   of course .            \n",
       "16   i do n't know .        \n",
       "17   i do n't know .        \n",
       "18   what are you doing ?   \n",
       "19   of course .            \n",
       "20   i do .                 \n",
       "21   i do n't know .        \n",
       "22   i do n't know .        \n",
       "23   i do n't know .        \n",
       "24   yes , of course .      \n",
       "25   not yet .              \n",
       "26   i do n't know .        \n",
       "27   i do n't know .        \n",
       "28   i do n't know .        \n",
       "29   of course .            \n",
       "..           ...            \n",
       "290  i am .                 \n",
       "291  i did .                \n",
       "292  i did .                \n",
       "293  of course .            \n",
       "294  of course .            \n",
       "295  yes , sir .            \n",
       "296  all right .            \n",
       "297  of course .            \n",
       "298  i do .                 \n",
       "299  yes , sir .            \n",
       "300  of course .            \n",
       "301  i do n't know .        \n",
       "302  i need your help .     \n",
       "303  of course .            \n",
       "304  i do n't know .        \n",
       "305  i do n't know .        \n",
       "306  oh , my god .          \n",
       "307  i do .                 \n",
       "308  i do n't know .        \n",
       "309  i do n't know .        \n",
       "310  oh , my god .          \n",
       "311  i do n't know .        \n",
       "312  not yet .              \n",
       "313  i do n't know .        \n",
       "314  i do n't know .        \n",
       "315  i do n't know .        \n",
       "316  oh , yeah .            \n",
       "317  i do n't know .        \n",
       "318  i do n't know .        \n",
       "319  i do n't know .        \n",
       "\n",
       "                                                                              MMI_VAD  \n",
       "0    i do n't know . i was just looking for my friend .                                \n",
       "1    i do n't know , but i did n't know why i was talking about it .                   \n",
       "2    i do n't know . i do n't know . i do n't understand .                             \n",
       "3    i do n't know . i do n't know , sweetheart .                                      \n",
       "4    i do n't think .                                                                  \n",
       "5    well , of course he does , but he does n't know .                                 \n",
       "6    i do n't know . i do n't know . it 's all .                                       \n",
       "7    no , i do n't know , but i do n't know , but i do n't know , but i                \n",
       "8    i do n't know . i do n't know what you 're talking about .                        \n",
       "9    i do n't know . i do n't know . i do n't want you .                               \n",
       "10   i do n't know . i 'm sorry .                                                      \n",
       "11   i do n't know , sweetheart .                                                      \n",
       "12   i do n't know . i do n't know . i do n't know . i do n't know . i                 \n",
       "13   i do n't know . i do n't want to talk .                                           \n",
       "14   i do n't know , but i can n't tell you .                                          \n",
       "15   i do n't know , sweetheart .                                                      \n",
       "16   i do n't know . i do n't know . that 's right .                                   \n",
       "17   i do n't know . i do n't understand .                                             \n",
       "18   i 'm sorry . i 'm fine . i 'm not sure .                                          \n",
       "19   that 's right . it 's all right . it 's all right .                               \n",
       "20   i do n't know . i do n't want you to be .                                         \n",
       "21   i do n't know . i do n't know .                                                   \n",
       "22   i do n't know . i do n't know .                                                   \n",
       "23   i do n't know . i do n't know . i can n't .                                       \n",
       "24   i do n't know . i do n't know . that 's right .                                   \n",
       "25   i do n't know .                                                                   \n",
       "26   i can n't . i can n't . i do n't .                                                \n",
       "27   i do n't know . i do n't know . i do n't know .                                   \n",
       "28   i do n't know . i do n't know . it 's just ...                                    \n",
       "29   i do n't know . i do n't know .                                                   \n",
       "..                               ...                                                   \n",
       "290  i do n't understand .                                                             \n",
       "291  i mean , i did n't .                                                              \n",
       "292  i do n't know . it 's all right .                                                 \n",
       "293  no , i 'm sorry . i 'm sorry . i 'm not sure .                                    \n",
       "294  no , i can n't . i do n't know .                                                  \n",
       "295  i do n't know . i do n't know . that 's ...                                       \n",
       "296  that 's right . it 's all right . it 's all right .                               \n",
       "297  i do n't know . i just ...                                                        \n",
       "298  i do n't want to be honest .                                                      \n",
       "299  i 'm not sure .                                                                   \n",
       "300  all right .                                                                       \n",
       "301  i do n't know .                                                                   \n",
       "302  i want you to be happy .                                                          \n",
       "303  no , i do n't know .                                                              \n",
       "304  well , it does n't matter . it 's all right . it 's all right .                   \n",
       "305  i do n't know what i want .                                                       \n",
       "306  i do n't know . i do n't want to be .                                             \n",
       "307  i do n't know . i do n't want to talk about it .                                  \n",
       "308  i do n't know . i do n't understand .                                             \n",
       "309  i do n't know anything .                                                          \n",
       "310  i do n't know . i do n't know . i 'm just saying .                                \n",
       "311  i do n't know . i 'm not sure .                                                   \n",
       "312  i do n't know . i do n't think so .                                               \n",
       "313  i do n't know . i do n't know .                                                   \n",
       "314  i do n't know . i do n't know . i do .                                            \n",
       "315  i do n't know . i do n't know . i do n't know anything .                          \n",
       "316  i can n't .                                                                       \n",
       "317  i do n't know . i do n't know . it 's a good man .                                \n",
       "318  i do n't know . i do n't know what you 're talking about . it 's just a few more  \n",
       "319  i do n't know . i do n't know . it 's just ...                                    \n",
       "\n",
       "[320 rows x 4 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame([inputs,targets,outputs1,outputs2]).T\n",
    "pred_df.columns = ['Input','Target','Basic','MMI_VAD']\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_S2S = []\n",
    "f = open('../pre-data/test/pred_S2S.txt', 'w', encoding = 'utf-8')\n",
    "N = (test_set['enc_input'].shape[0] // args.batch_size) * args.batch_size\n",
    "for i in range(N):\n",
    "    f.write('HISTORY:\\n')\n",
    "    uttr = ids_to_sentence(test_set['enc_input'][i,:], test_set['enc_input_len'][i], id2token)\n",
    "    f.write('- {}\\n'.format(uttr))\n",
    "    f.write('LABEL:\\n')\n",
    "    label = ids_to_sentence(test_set['target'][i,:], test_set['dec_input_len'][i], id2token)\n",
    "    f.write('- {}\\n'.format(label))\n",
    "    f.write('PREDICTION:\\n')\n",
    "    pred = ids_to_sentence(prediction[i//args.batch_size][i%args.batch_size,:,0], None, id2token)\n",
    "    f.write('- {}\\n\\n'.format(pred))\n",
    "    pred_S2S.append(pred)\n",
    "f.close()\n",
    "with open('../pre-data/test/pred_S2S.pickle', 'wb') as f:\n",
    "    pickle.dump(pred_S2S, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pre-data/test/prediction.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
