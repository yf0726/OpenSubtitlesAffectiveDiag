{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "from model import Options, Seq2SeqAttn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beautifultable import BeautifulTable\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the command line arguments.\n",
    "save_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/input/'\n",
    "output_dir = '/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', type = str, default = save_dir,\n",
    "                    help = 'the directory to the data')\n",
    "\n",
    "parser.add_argument('--word_embeddings_path', type = str, default = save_dir+'word_embeddings.npy',\n",
    "                    help = 'the directory to the pre-trained word embeddings')\n",
    "parser.add_argument('--VAD_path', type = str, default = save_dir+'VAD.npy',\n",
    "                    help = 'the directory to VAD')\n",
    "parser.add_argument('--tf_path', type = str, default = save_dir+'tf.npy',\n",
    "                    help = 'the directory to term frequency')\n",
    "parser.add_argument('--VAD_loss_path', type = str, default = save_dir+'VAD_loss.npy',\n",
    "                    help = 'the directory to VAD loss for each word')\n",
    "parser.add_argument('--ti_path', type = str, default = save_dir+'mu_li.npy',\n",
    "                    help = 'the directory to term importance')\n",
    "\n",
    "parser.add_argument('--num_epochs', type = int, default = 3,\n",
    "                    help = 'the number of epochs to train the data')\n",
    "parser.add_argument('--batch_size', type = int, default = 64,\n",
    "                    help = 'the batch size')\n",
    "parser.add_argument('--learning_rate', type = float, default = 0.001,\n",
    "                    help = 'the learning rate')\n",
    "parser.add_argument('--beam_width', type = int, default = 32,\n",
    "                    help = 'the beam width when decoding')\n",
    "parser.add_argument('--word_embed_size', type = int, default = 300,\n",
    "                    help = 'the size of word embeddings')\n",
    "parser.add_argument('--n_hidden_units_enc', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of encoder')\n",
    "parser.add_argument('--n_hidden_units_dec', type = int, default = 256,\n",
    "                    help = 'the number of hidden units of decoder')\n",
    "parser.add_argument('--attn_depth', type = int, default = 128,\n",
    "                    help = 'attention depth')\n",
    "\n",
    "parser.add_argument('--restore_path_TS', type = str, default = output_dir+'model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_TS', type = str, default = output_dir+'/model_dailydialog_rf/model_TS',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_path_ST', type = str, default = output_dir+'model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to restore the trained model')\n",
    "parser.add_argument('--save_path_ST', type = str, default = output_dir+'/model_dailydialog_rf/model_ST',\n",
    "                    help = 'the path to save the trained model to')\n",
    "\n",
    "parser.add_argument('--restore_epoch', type = int, default = 3,\n",
    "                    help = 'the epoch to restore')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    def load_np_files(path):\n",
    "        my_set = {}\n",
    "        my_set['enc_input'] = np.load(os.path.join(path, 'enc_input.npy'))\n",
    "        my_set['dec_input'] = np.load(os.path.join(path, 'dec_input.npy'))\n",
    "        my_set['target'] = np.load(os.path.join(path, 'target.npy'))\n",
    "        my_set['enc_input_len'] = np.load(os.path.join(path, 'enc_input_len.npy'))\n",
    "        my_set['dec_input_len'] = np.load(os.path.join(path, 'dec_input_len.npy'))\n",
    "        \n",
    "        # to check if or not to complete the last batch\n",
    "        idx = np.arange(my_set['dec_input'].shape[0])\n",
    "        left_samples = idx[-1]%args.batch_size\n",
    "        if left_samples:\n",
    "            last_batch_idx = np.random.randint(0,idx[-1]-left_samples,size = args.batch_size - left_samples - 1)\n",
    "            idx = np.concatenate([idx,last_batch_idx])\n",
    "            \n",
    "            my_set['enc_input'] = my_set['enc_input'][idx]\n",
    "            my_set['dec_input'] = my_set['dec_input'][idx]\n",
    "            my_set['target'] = my_set['target'][idx]\n",
    "            my_set['enc_input_len'] = my_set['enc_input_len'][idx]\n",
    "            my_set['dec_input_len'] = my_set['dec_input_len'][idx]\n",
    "        return my_set\n",
    "    test_set = load_np_files(os.path.join(data_path, 'test'))\n",
    "    # dictionary index of words\n",
    "    with open(os.path.join(data_path, 'token2id.pickle'), 'rb') as file:\n",
    "        token2id = pickle.load(file)\n",
    "    with open(os.path.join(data_path, 'id2token.pickle'), 'rb') as file:\n",
    "        id2token = pickle.load(file)\n",
    "    return test_set, token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_sentence(ids, uttr_len, id2token): # ?\n",
    "    tokens = []\n",
    "    if uttr_len is not None:\n",
    "        for i in range(uttr_len):\n",
    "            if id2token[ids[i]] != '<eos>' and id2token[ids[i]] != '<go>':\n",
    "                tokens.append(id2token[ids[i]])\n",
    "    else:\n",
    "        i = 0\n",
    "        while i < len(ids) and id2token[ids[i]] != '<eos>':\n",
    "            tokens.append(id2token[ids[i]])\n",
    "            i += 1\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert(myset):\n",
    "    enc_input = myset['dec_input'][:,1:]\n",
    "    dec_input = np.insert(myset['enc_input'], 0, token2id['<go>'], axis=1) # add <go> in the beginning of encoder\n",
    "\n",
    "    target = np.insert(myset['enc_input'], -1, 0, axis=1) \n",
    "    tmp_idx = [np.where(s==0)[0][0] for s in target] \n",
    "    target[np.arange(target.shape[0]),tmp_idx] = token2id['<eos>'] # add <eos> at the end of encoder\n",
    "    \n",
    "    newset = {}\n",
    "    \n",
    "    newset['enc_input'] = enc_input\n",
    "    newset['dec_input'] = dec_input\n",
    "    newset['target'] = target\n",
    "    newset['enc_input_len'] = myset['dec_input_len']\n",
    "    newset['dec_input_len'] = myset['enc_input_len']\n",
    "    return newset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given enc_input predict prediction P(T|S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, token2id, id2token = read_data(args.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9984"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set['enc_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "Restoring a pre-trained model from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_TS/model_epoch_003.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_TS/model_epoch_003.ckpt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     test_set, token2id, id2token = read_data(args.data_path)\n",
    "    max_uttr_len_enc = test_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = test_set['dec_input'].shape[1]\n",
    "    \n",
    "    test_set['enc_input'] = test_set['enc_input'][:5*args.batch_size]\n",
    "\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "    VAD_loss = np.load(args.VAD_loss_path)\n",
    "    VAD_loss = VAD_loss.reshape(-1,1)\n",
    "\n",
    "    options = Options(mode = 'PREDICT',\n",
    "                      VAD_mode = 'FALSE',\n",
    "                      num_epochs = args.num_epochs,\n",
    "                      batch_size = args.batch_size,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_TS = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_TS.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    model_TS.restore(os.path.join(args.restore_path_TS, 'model_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    prediction_TS,probability_TS = model_TS.predict(test_set['enc_input'], test_set['enc_input_len'],VAD,termfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 21, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.4579206,  -6.605519 ,  -6.68667  , ..., -18.483994 ,\n",
       "        -18.982075 , -25.699299 ],\n",
       "       [ -5.67196  ,  -7.569267 ,  -7.5700526, ..., -21.53505  ,\n",
       "        -21.718372 , -22.024775 ],\n",
       "       [ -6.3648877,  -7.661137 ,  -7.7334   , ..., -18.038395 ,\n",
       "        -18.908665 , -28.184574 ],\n",
       "       ...,\n",
       "       [ -6.554825 ,  -7.3182216,  -7.3226833, ..., -19.175753 ,\n",
       "        -23.30402  , -27.547388 ],\n",
       "       [ -6.267722 ,  -7.088844 ,  -7.1873302, ..., -26.72459  ,\n",
       "        -29.60926  , -31.220829 ],\n",
       "       [ -6.704087 ,  -6.955397 ,  -7.663645 , ..., -18.191755 ,\n",
       "        -18.73783  , -24.330801 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/prediction_TS.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction_TS, f)\n",
    "    \n",
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/probability_TS.pickle', 'wb') as f:\n",
    "    pickle.dump(probability_TS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "given prediction predict enc_input P(T|S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pre-data/test/prediction_TS_noVAD.pickle', 'rb') as file:\n",
    "    prediction_TS = pickle.load(file)\n",
    "    \n",
    "with open('../pre-data/test/probability_TS_noVAD.pickle', 'rb') as file:\n",
    "    probability_TS = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every i in range(prediction_TS.shape[0])\n",
    "# enc_input[i:i+args.beam_width] is the prediction of top args.beam_width of one given source from model_TS\n",
    "enc_input = prediction_TS[0,:,:].T\n",
    "for i in range(prediction_TS.shape[0]-1):\n",
    "    enc_input = np.concatenate((enc_input,prediction_TS[i+1,:,:].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 21, 32) (10240, 21)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_TS.shape,enc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set = {}\n",
    "new_test_set['enc_input'] = enc_input\n",
    "\n",
    "multi_idx = np.tile(np.arange(test_set['enc_input'].shape[0]).T,(args.beam_width,1)).T.ravel()\n",
    "\n",
    "new_test_set['dec_input'] = np.insert(test_set['enc_input'][multi_idx], 0, token2id['<go>'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.insert(test_set['enc_input'], -1, 0, axis=1) \n",
    "tmp_idx = [np.where(s==0)[0][0] for s in target] \n",
    "target[np.arange(target.shape[0]),tmp_idx] = token2id['<eos>'] # add <eos> at the end of encoder\n",
    "\n",
    "new_test_set['target'] = target[multi_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set['enc_input_len'] = (enc_input.shape[1]*np.ones(enc_input.shape[0])).astype(int)\n",
    "_,idx = np.unique(np.argwhere(enc_input==token2id['<eos>'])[:,0],return_index=True)\n",
    "# multi <eos> in one sentence, so find the first <eos> in each row\n",
    "# for those predictions without <eos> the length is max_len\n",
    "new_test_set['enc_input_len'][np.argwhere(enc_input==token2id['<eos>'])[idx,0]] = np.argwhere(enc_input==token2id['<eos>'])[idx,1]\n",
    "\n",
    "new_test_set['dec_input_len'] = np.tile(test_set['enc_input_len'],(args.beam_width,1)).T.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the TensorFlow graph...\n",
      "embedding/embedding:0\n",
      "encoding/rnn/gru_cell/gates/kernel:0\n",
      "encoding/rnn/gru_cell/gates/bias:0\n",
      "encoding/rnn/gru_cell/candidate/kernel:0\n",
      "encoding/rnn/gru_cell/candidate/bias:0\n",
      "decoding/memory_layer/kernel:0\n",
      "decoding/attention_v:0\n",
      "decoding/my_bahdanau_attention/query_layer/kernel:0\n",
      "decoding/my_bahdanau_attention/attention_Wb/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/gates/bias:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/kernel:0\n",
      "decoding/attention_wrapper/gru_cell/candidate/bias:0\n",
      "decoding/dense/kernel:0\n",
      "decoding/dense/bias:0\n",
      "Restoring a pre-trained model from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_ST/model_epoch_003.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from /Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/model_dailydialog_rf/model_ST/model_epoch_003.ckpt\n",
      "Start to train the model...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     test_set, token2id, id2token = read_data(args.data_path)\n",
    "    word_embeddings = np.load(args.word_embeddings_path)\n",
    "    VAD = np.load(args.VAD_path)\n",
    "    termfreq = np.load(args.ti_path) # term importance\n",
    "    termfreq = termfreq.reshape(-1,1)\n",
    "\n",
    "    max_uttr_len_enc = new_test_set['enc_input'].shape[1]\n",
    "    max_uttr_len_dec = new_test_set['dec_input'].shape[1]\n",
    "    \n",
    "    options = Options(mode = 'POST_PREDICT',\n",
    "                      VAD_mode = 'FALSE',\n",
    "                      num_epochs = 1,\n",
    "                      batch_size = 1,\n",
    "                      learning_rate = args.learning_rate,\n",
    "                      beam_width = args.beam_width,\n",
    "                      corpus_size = len(token2id),\n",
    "                      max_uttr_len_enc = max_uttr_len_enc,\n",
    "                      max_uttr_len_dec = max_uttr_len_dec,\n",
    "                      go_index = token2id['<go>'],\n",
    "                      eos_index = token2id['<eos>'],\n",
    "                      word_embed_size = args.word_embed_size,\n",
    "                      n_hidden_units_enc = args.n_hidden_units_enc,\n",
    "                      n_hidden_units_dec = args.n_hidden_units_dec,\n",
    "                      attn_depth = args.attn_depth,\n",
    "                      word_embeddings = word_embeddings)\n",
    "    model_ST = Seq2SeqAttn(options)\n",
    "\n",
    "    for var in model_ST.tvars:\n",
    "        print(var.name)\n",
    "\n",
    "    model_ST.restore(os.path.join(args.restore_path_ST, 'model_epoch_{:03d}.ckpt'.format(args.restore_epoch)))\n",
    "    probability_ST = model_ST.post_predict(new_test_set, VAD,termfreq)\n",
    "    probability_ST = probability_ST.reshape(-1,args.beam_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yan/Documents/document/EPFL/MA2/semesterprj/code/seq2seq_attn/affect-rich/output/prediction/probability_ST.pickle', 'wb') as f:\n",
    "    pickle.dump(probability_ST, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_VAD(sentence,VAD,s_len):\n",
    "    vad = 0\n",
    "    for i in range(s_len):\n",
    "        vad += sum(abs(VAD[sentence[i]]))\n",
    "    return vad/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMI_bidi(pred_TS,prob_TS,prob_ST,VAD,id2token):\n",
    "    \"\"\"\n",
    "    pred_TS: [num_sentence, max_uttr_len_dec, beam_width]\n",
    "    prob_TS, prob_ST: [num_sentence, beam_width]\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['label_num','label','target',\"prediction\", \"prob_TS\", \"prob_ST\",'ABS_VAD'])\n",
    "    \n",
    "    for sentence_num in range(pred_TS.shape[0]):\n",
    "        bias = sentence_num*args.beam_width\n",
    "        target = ids_to_sentence(test_set['dec_input'][sentence_num], test_set['dec_input_len'][sentence_num], id2token)\n",
    "        for i in range(args.beam_width):\n",
    "            label = ids_to_sentence(new_test_set['dec_input'][bias+i], new_test_set['dec_input_len'][bias+i]+1, id2token)\n",
    "            pred_s = ids_to_sentence(pred_TS[sentence_num,:,i], new_test_set['enc_input_len'][bias+i], id2token)\n",
    "            vad = sentence_VAD(pred_TS[sentence_num,:,i], VAD, new_test_set['enc_input_len'][bias+i])\n",
    "            df.loc[bias+i] = list([sentence_num,label,target,pred_s,prob_TS[sentence_num,i],\n",
    "                              prob_ST[sentence_num,i],vad])    \n",
    "#     print(table)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_VAD = MMI_bidi(prediction_TS,probability_TS,probability_ST,VAD,id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAD.to_csv('MMI_VAD.csv',index=False)\n",
    "df = pd.read_csv('MMI.csv')\n",
    "df_VAD = pd.read_csv('MMI_VAD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs1 = []\n",
    "outputs2 = []\n",
    "\n",
    "for i in range(int(len(df)/32)):\n",
    "    input_ = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_TS',ascending=False).head(10).iloc[0].label\n",
    "    TS_response = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_TS',ascending=False).head(10).iloc[0].prediction\n",
    "    ST_response = df_VAD.loc[32*i:32*i+31].sort_values(by='prob_ST',ascending=False).head(10).sort_values(by=\"ABS_VAD\",ascending=False).iloc[0].prediction\n",
    "    \n",
    "    inputs.append(input_)\n",
    "    outputs1.append(TS_response)\n",
    "    outputs2.append(ST_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_S2S = []\n",
    "f = open('../pre-data/test/pred_S2S.txt', 'w', encoding = 'utf-8')\n",
    "N = (test_set['enc_input'].shape[0] // args.batch_size) * args.batch_size\n",
    "for i in range(N):\n",
    "    f.write('HISTORY:\\n')\n",
    "    uttr = ids_to_sentence(test_set['enc_input'][i,:], test_set['enc_input_len'][i], id2token)\n",
    "    f.write('- {}\\n'.format(uttr))\n",
    "    f.write('LABEL:\\n')\n",
    "    label = ids_to_sentence(test_set['target'][i,:], test_set['dec_input_len'][i], id2token)\n",
    "    f.write('- {}\\n'.format(label))\n",
    "    f.write('PREDICTION:\\n')\n",
    "    pred = ids_to_sentence(prediction[i//args.batch_size][i%args.batch_size,:,0], None, id2token)\n",
    "    f.write('- {}\\n\\n'.format(pred))\n",
    "    pred_S2S.append(pred)\n",
    "f.close()\n",
    "with open('../pre-data/test/pred_S2S.pickle', 'wb') as f:\n",
    "    pickle.dump(pred_S2S, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pre-data/test/prediction.pickle', 'wb') as f:\n",
    "    pickle.dump(prediction, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
