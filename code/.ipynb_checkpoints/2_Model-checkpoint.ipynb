{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim tutorial:\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. set up dictionary for the corpus\n",
    "2. token2id, id2token\n",
    "3. extract questions and answers: question mark; and sentences less than 20 words\n",
    "4. word2vec\n",
    "\n",
    "convert to lower case; but no expansion\n",
    "try not converting lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc_dict\n",
    "\n",
    "dec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "import os\n",
    "import chardet   #需要导入这个模块，检测编码格式\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(dict_, text):\n",
    "    # Create a regular expression  from the dictionary keys\n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict_.keys())))\n",
    "    # For each match, look-up corresponding value in dictionary\n",
    "    return regex.sub(lambda mo: dict_[mo.string[mo.start():mo.end()]], text) \n",
    "\n",
    "dict_sub = {'\\\"':'\\'','´':'\\'',\n",
    "            '\\'s':' \\'s','\\'m':' \\'m','\\'ve':' \\'ve','\\'re':' \\'re','\\'ll':' \\'ll','\\'d':' \\'d'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_extract(line,dict_=dict_sub):\n",
    "    encode_type = chardet.detect(line)  \n",
    "    line = line.decode(encode_type['encoding']) #进行相应解码，赋给原标识符（变量）\n",
    "    line = multiple_replace(dict_, line)\n",
    "    # line = line.lower().replace('<eos>','\\n').split('\\n')\n",
    "    line = line.replace('<EOS>','\\n').split('\\n')\n",
    "    line = list(filter(None,line))\n",
    "    \n",
    "    idx = [i for i,x in enumerate(line) if '?' in x]\n",
    "    diag_list = []\n",
    "    for i in idx:\n",
    "        if (i < len(line)-1):\n",
    "            if (len(line[i].split())<20) & (len(line[i+1].split())<20):\n",
    "                diag_list.append(line[i])\n",
    "                diag_list.append('<go> '+line[i+1]+' <eos>')\n",
    "                # diag_list.append(line[i+1])\n",
    "    return diag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in smart_open(os.path.join(self.dirname, fname), 'rb'):\n",
    "                yield from sentence_extract(line)\n",
    "                # return sentence_extract(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus('./data/')\n",
    "sentences = [x.split() for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Word2Vec(sentences, min_count=1,size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1571 unique tokens: ['<NUM>-type', '?', 'Why', 'a', 'did']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1270"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[0]\n",
    "id2token = dictionary.id2token\n",
    "token2id = dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/semester/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = []\n",
    "for i in range(len(dictionary)):\n",
    "    word_embeddings.append(model[id2token[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_token2id = []\n",
    "for i in range(len(sentences)):\n",
    "    sentence_token2id.append([token2id.get(item,item) for item in sentences[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = sentence_token2id[::2]\n",
    "dec_input = [x[:-1] for x in sentence_token2id[1::2]] # '<go>' + \n",
    "target = [x[1:] for x in sentence_token2id[1::2]] # + '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 20 20\n"
     ]
    }
   ],
   "source": [
    "max_uttr_len_enc = max([len(x) for x in enc_input])\n",
    "max_uttr_len_dec = max([len(x) for x in dec_input])\n",
    "max_uttr_len_target = max([len(x) for x in target])\n",
    "print(max_uttr_len_enc,max_uttr_len_dec,max_uttr_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = np.array([i + [0]*(max_uttr_len_enc-len(i)) for i in enc_input])\n",
    "dec_input = np.array([i + [0]*(max_uttr_len_dec-len(i)) for i in dec_input])\n",
    "target = np.array([i + [0]*(max_uttr_len_target-len(i)) for i in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 20)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_test, dec_train, dec_test, target_train, target_test = train_test_split(\n",
    "    enc_input, dec_input, target, test_size=0.2, random_state=1)\n",
    "\n",
    "enc_test, enc_val, dec_test, dec_val,target_test,target_val = train_test_split(\n",
    "    enc_test, dec_test, target_test, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dec_input: `<go>` + answer\n",
    "target: answer + `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./seq2seq_attn/pre-data/word_embeddings.npy',word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_train]\n",
    "dec_input_len = [sum(x!=0) for x in dec_train]\n",
    "np.save('./seq2seq_attn/pre-data/train/enc_input.npy',enc_train)\n",
    "np.save('./seq2seq_attn/pre-data/train/dec_input.npy',dec_train)\n",
    "np.save('./seq2seq_attn/pre-data/train/target.npy',target_train)\n",
    "np.save('./seq2seq_attn/pre-data/train/enc_input_len.npy',enc_input_len)\n",
    "np.save('./seq2seq_attn/pre-data/train/dec_input_len.npy',dec_input_len)\n",
    "\n",
    "enc_input_len = [sum(x!=0) for x in enc_val]\n",
    "dec_input_len = [sum(x!=0) for x in dec_val]\n",
    "np.save('./seq2seq_attn/pre-data/validation/enc_input.npy',enc_val)\n",
    "np.save('./seq2seq_attn/pre-data/validation/dec_input.npy',dec_val)\n",
    "np.save('./seq2seq_attn/pre-data/validation/target.npy',target_val)\n",
    "np.save('./seq2seq_attn/pre-data/validation/enc_input_len.npy',enc_input_len)\n",
    "np.save('./seq2seq_attn/pre-data/validation/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_len = [sum(x!=0) for x in enc_test]\n",
    "dec_input_len = [sum(x!=0) for x in dec_test]\n",
    "np.save('./seq2seq_attn/pre-data/test/enc_input.npy',enc_test)\n",
    "np.save('./seq2seq_attn/pre-data/test/dec_input.npy',dec_test)\n",
    "np.save('./seq2seq_attn/pre-data/test/target.npy',target_test)\n",
    "np.save('./seq2seq_attn/pre-data/test/enc_input_len.npy',enc_input_len)\n",
    "np.save('./seq2seq_attn/pre-data/test/dec_input_len.npy',dec_input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2token,open('./seq2seq_attn/pre-data/id2token.pickle','wb'))\n",
    "pickle.dump(token2id,open('./seq2seq_attn/pre-data/token2id.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('id2token.pickle', 'rb') as file:\n",
    "#     test = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semester)",
   "language": "python",
   "name": "semester"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
